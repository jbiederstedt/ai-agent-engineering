{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "# Using complex Memory Schemas for advanced operations\n",
    "* In the previous exercise we saved the LT memory as a string. In this one, we will learn how to use a more sophisticated schema and make use of it for advanced operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1787e-14c1-428c-bd98-de0088a203a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 026-profile-schema.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0eb873-d417-43db-992d-312dc428d687",
   "metadata": {},
   "source": [
    "## Defining the format of our User Profile memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440470ff-73a1-4398-a919-5bef46c8b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class UserProfile(TypedDict):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str  # The user's preferred name\n",
    "    interests: List[str]  # A list of the user's interests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d701053-c272-4e33-bf69-6c6e6b0b889d",
   "metadata": {},
   "source": [
    "## Let's explain what we just did\n",
    "\n",
    "This code defines a **custom data structure** called `UserProfile` using Python's `TypedDict`. Here's a breakdown in simple terms:\n",
    "\n",
    "1. **Purpose**: It describes how a user profile should be organized and what kind of data it should contain. This helps make the code clearer and prevents errors.\n",
    "\n",
    "2. **Import Statements**:  \n",
    "   - `TypedDict` is imported to create a structured dictionary with specific types for each value.  \n",
    "   - `List` is imported to specify that one of the fields will contain a list of items.\n",
    "\n",
    "3. **Defining `UserProfile`:**  \n",
    "   - It is a **dictionary-like structure** where keys and their corresponding value types are predefined.  \n",
    "   - The `user_name` key must contain a **string** (str).  \n",
    "   - The `interests` key must contain a **list of strings** (List[str]).\n",
    "\n",
    "4. **Comment Explanation**:  \n",
    "   - Each field has a comment explaining its purpose:\n",
    "     - `user_name`: The preferred name of the user.  \n",
    "     - `interests`: A list of things the user is interested in.\n",
    "\n",
    "#### Example Usage:\n",
    "```python\n",
    "profile: UserProfile = {\n",
    "    \"user_name\": \"Alice\",\n",
    "    \"interests\": [\"coding\", \"reading\", \"music\"]\n",
    "}\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- `\"user_name\"` is a string, `\"Alice\"`.  \n",
    "- `\"interests\"` is a list of strings, `[\"coding\", \"reading\", \"music\"]`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3153f-7448-43cb-ac36-a49bef706c25",
   "metadata": {},
   "source": [
    "## Let's now use the previous User Profile schema to save the data of our first user in the Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec4ee4a-ebd3-46ea-a7b2-22f654fdeb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypedDict instance\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"AI\", \"SF\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bd2336-050b-46e4-9bcc-ebe61de04262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = \"user_profile\"\n",
    "value = user_profile\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cd631-4c71-4cad-8731-a0f3516629f3",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "The previous code builds on the earlier `UserProfile` example and demonstrates how to **store user data in memory** using the `InMemoryStore` class from LangGraph. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Create a User Profile\n",
    "```python\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"AI\", \"SF\"]\n",
    "}\n",
    "```\n",
    "- A **user profile** dictionary is created based on the `UserProfile` structure.  \n",
    "- It has:\n",
    "  - **user_name** = \"Julio\"  \n",
    "  - **interests** = [\"vespa\", \"AI\", \"SF\"]\n",
    "\n",
    "#### Import Required Modules\n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "```\n",
    "- **`uuid`**: Useful for generating unique identifiers (not used in this snippet yet, but might be used later).  \n",
    "- **`InMemoryStore`**: A tool provided by LangGraph to **store data temporarily** in memory.  \n",
    "\n",
    "#### Initialize Memory Storage\n",
    "```python\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- An **instance** of `InMemoryStore` is created to **store data in memory**.\n",
    "\n",
    "#### Define a Namespace for Storage\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "```\n",
    "- **user_id = \"1\"**: Represents the ID of the user (e.g., Julio).  \n",
    "- **namespace_for_memory = (user_id, \"memory\")**: Combines the user ID and a label (\"memory\") to create a **unique namespace** for storing the user's data.  \n",
    "\n",
    "#### Save Data in Memory\n",
    "```python\n",
    "key = \"user_profile\"\n",
    "value = user_profile\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- **key = \"user_profile\"**: Specifies a **label** for the stored data.  \n",
    "- **value = user_profile**: Stores the earlier-defined `user_profile`.  \n",
    "- **`put()`**: Saves the data in memory using:\n",
    "  - **Namespace**: `(user_id, \"memory\")` – Keeps data organized per user or context.  \n",
    "  - **Key-Value Pair**: `\"user_profile\"` → `{user_name: \"Julio\", interests: [...]}`.\n",
    "\n",
    "#### What Happens in Memory?\n",
    "The data is stored like this:\n",
    "```\n",
    "{\n",
    "  (\"1\", \"memory\"): {\n",
    "      \"user_profile\": {\n",
    "          \"user_name\": \"Julio\",\n",
    "          \"interests\": [\"vespa\", \"AI\", \"SF\"]\n",
    "      }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc37b6d-8e7d-438f-a7b8-f30288828e7f",
   "metadata": {},
   "source": [
    "## OK. Let's now see how to retrieve this information from the Memory Store.\n",
    "* As we learned in the previous exercise, we can use [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) to retrieve objects from the store by namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783b5ccf-f6b3-4586-9861-2fc5a8e60ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'user_name': 'Julio', 'interests': ['vespa', 'AI', 'SF']}, 'key': 'user_profile', 'namespace': ['1', 'memory'], 'created_at': '2025-01-07T08:48:22.047331+00:00', 'updated_at': '2025-01-07T08:48:22.047333+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa22bdc-185a-463a-b226-942a77c24df9",
   "metadata": {},
   "source": [
    "* As you remember, we can also use [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) to retrieve a specific object by namespace and key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a893a54-709a-4c9e-96dc-dbe7f524e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'interests': ['vespa', 'AI', 'SF']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the memory by namespace and key\n",
    "profile = in_memory_store.get(namespace_for_memory, \"user_profile\")\n",
    "profile.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c65503-3002-4d5c-8093-205797cb3720",
   "metadata": {},
   "source": [
    "## If we bind the UserProfile schema with the LLM model, our app will respond with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780e6e3b-c9e3-4fe8-bab8-afb70f1b1c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'interests': ['driving vespa']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# PAY ATTENTION HERE: see how we use the UserProfile schema\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(UserProfile)\n",
    "\n",
    "# PAY ATTENTION HERE: Using the bound model, now the app \n",
    "# will respond with structured output.\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "structured_output = model_with_structure.invoke([HumanMessage(\"My name is Julio, I like to drive my vespa.\")])\n",
    "structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64bc1b-c2ab-4939-b0f9-f65d2b59f8aa",
   "metadata": {},
   "source": [
    "## Let's review what we just did\n",
    "\n",
    "The previous code builds on the previous example and shows how to **use AI to process text input and generate structured data** that matches the `UserProfile` schema. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Import Required Libraries\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "\n",
    "- **`pydantic`**: Helps define and validate data structures (not directly used here but is compatible with the schema).  \n",
    "- **`HumanMessage`**: Represents a message written by a human, simulating user input.  \n",
    "- **`ChatOpenAI`**: Connects to OpenAI's GPT model for generating responses.\n",
    "\n",
    "#### Initialize the Chat Model\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **`ChatOpenAI`**: Initializes an AI model (GPT-4o).  \n",
    "- **`model=\"gpt-4o\"`**: Specifies the version of the AI model to use.  \n",
    "- **`temperature=0`**: Controls randomness—0 means the output is **consistent and predictable**.\n",
    "\n",
    "#### Bind Schema to the Model\n",
    "```python\n",
    "model_with_structure = model.with_structured_output(UserProfile)\n",
    "```\n",
    "- **Purpose**: Ensures the model's output matches the **`UserProfile` schema** defined earlier.  \n",
    "- **Key Idea**: Forces the AI to **format its response as structured data** (dictionary) with fields like:\n",
    "  - `user_name` (string)  \n",
    "  - `interests` (list of strings).\n",
    "\n",
    "#### Process Human Input\n",
    "```python\n",
    "structured_output = model_with_structure.invoke(\n",
    "    [HumanMessage(\"My name is Julio, I like to drive my vespa.\")]\n",
    ")\n",
    "```\n",
    "- **Input**: Simulates a human message: `\"My name is Julio, I like to drive my vespa.\"`  \n",
    "- **`invoke()`**: Processes the input using the AI model and outputs data that fits the schema.  \n",
    "- **Structured Output**: The AI analyzes the text and returns the data in the required format.\n",
    "\n",
    "#### What Does the Output Look Like?\n",
    "The output is formatted as a **valid `UserProfile` dictionary**, like this:\n",
    "```python\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### How Does It Work?\n",
    "1. **AI Processes the Message**:  \n",
    "   - Extracts the name (\"Julio\").  \n",
    "   - Identifies the interest (\"vespa\").  \n",
    "\n",
    "2. **Schema Validation**:  \n",
    "   - Ensures the output matches the `UserProfile` structure.  \n",
    "   - Returns structured data, not plain text.\n",
    "\n",
    "#### Example Use Case\n",
    "Imagine building a chatbot that collects user profiles for recommendations. Instead of manually parsing user messages, the AI automatically extracts and structures the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa593b-ca99-4df0-b3e5-ba4fcf47677d",
   "metadata": {},
   "source": [
    "## We can now add this change to the chatbot with long-term memory we built in the previous exercise\n",
    "* PAY ATTENTION: see how we use the `new_memory` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe43631-56f8-45c0-abfe-1d3e65f7f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAAFNCAIAAABt7QHtAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdcE+f/wJ/sXCYQ9pLpYggIFsWKC6vUvQvUgf3VWmunbW1rW6t11FaljlZbC9qKo+5RVERFRQVXUapfB1VEQEbIIju55PfH2ZRqQDS5O+649x+8yOXuuU/yznN3z93neR6a1WoFFESGjncAFI5CKSQ8lELCQykkPJRCwkMpJDxMvAMADVUGjcqsVcEmo8Wgs+AdTpvgQHQGi8YXMnlChlcnLr7B0PBqF977S323THPvuiawC8+gs/BEDFcvttlAjEYqG6LLa42aJjODSbt/QxscyQ+J5IfHCXEJBgeFf19TnzvY6BPM9Q2FgiP4kICBcQDOxWS03PtLU3FDU3lT22eEe/cXRBgHgKlCo8Fy7LdaGp3WZ4TExYON2X6xQaeGzx2USqsNQ6Z4u3pi9+mwU/jwnu7A+pqxc/w9/DnY7BEXlFLTwZ9qElMlYTECbPaIkUJZnfHE9vrx7/hjsK/2wOFNDyP7iAM68zDYFxYK713XXC6QjX8nAO0dtSvysh/6h0PRL7qgvSPU24VNctOpXQ0dzR8AIDXTp7xUXf23Du0doa7w+Pb6tHkdzh/C2Dn+l4/J9RozqntBV+GFozKfIC6bQ+xmgyOExwmK9jeiugsUFZpNlssF8heGSdDbRfunWy9R7X29vM6I3i5QVHjlhDx5vAd65ROFfmM8rhUp0SsfRYU3ipsCOkPold8cGIZLS0ufe3O1Wn3z5k2nRvQvgV15ZUVK9K780VIorTZweHShKwul8h9j0aJFS5Ysee7NJ0+evH//fqdG9B+CI/n3/tKgVDhaCh/c1nbpid1tX4PB8HwbIpXDaETxXAUACIvh19xFq3WBXi008kSoXIgWFRVNmjQpKSlpwoQJO3bsAAAsWLDg2LFjd+/ejY+Pj4+Pr6mpAQAcOHAgIyMjMTFx4MCBn332mVwuRzYvKCiIj48vLCycMWNGYmLi+vXrhw8fLpPJdu7cGR8fP3z4cDRiFrqw6iqf80f2VNB6XqhRmfki5xeu1Wo//vjjkJCQ+fPnl5eXNzQ0AAAyMzPr6uqqq6sXLlwIAHB3dwcAlJWVBQUFpaamymSy7du3azSarKwsWznffPPN7NmzZ82aFRgYmJyc/NZbb/Xs2TM9PZ3NRuX2NE/E0KpgNEomnkKZTGYwGAYOHDhs2DDbwsDAQBcXl8bGxpiYGNvCTz/9lEajIf8zmczs7GyDwcDhPLrDPmnSJFuF8/T0ZDKZ7u7uzTd3LnwxU6NEq4GPlkImm05HoWw/P7/o6OhffvkFgqCxY8e2UmlMJtP27dvz8vJqa2u5XK7FYpHL5d7e3si7vXr1cn5wLUNn0Dg8utVqtf2qnFm400tEYLFpGoXzDx00Gm316tXDhw/PysoaO3bslStX7K5mtVrffffd7OzskSNHrl27NjU1FQBgsfyb1cHjYfEMwYZGaabTaWj4Q1EhX8TUqFA5dAgEgnnz5u3evVsgELz//vtarRZZ3rzhdeXKlQsXLsybNy8tLS0yMjIsLOypxaL6xEarglG6uENRocSXbdSjksuEtB/8/PwmT56sVquR608IghobG231TKFQAAC6du3a/GXzWvgYEARJpVI0okXQaWDvILSypNA6F/qGQOcPNUb2ETu3WJPJNG7cuJSUlNDQ0J07dwoEAn9/fwBAXFzcgQMHlixZEhMTIxKJoqKi2Gz22rVrx4wZc+fOnZycHABAeXk5svKTxMbGHjlyZNOmTSKRKDo6ui219pm482dTp65855ZpA61a6BsCNT40GnROPh3qdLqEhITDhw8vW7aMxWJlZWVxuVwAQGpq6sSJE48dO7ZmzZpr1655enouXrz45s2bH330UUlJyYYNG/r27bt9+/aWin377bfj4+M3btyYk5Pz4MED58YMAKi4rg2KQOvsi+JT+7MHpF6dOGE98EnNaz88rNBdP6canOaFUvkopgJH9RXvXVvdisITJ04gjfHH4HA4Ld0wy8nJCQ4OdmqYj6NWq1u6R+Pq6mq7y9OcFStW9OzZs6UCiw/Jeg11c2qM/wHd3JmTO+s9fDmRSfbPiDqdzu43YjQaW2rwIc1wZ4f5HywWS21trd23TCYTi2Xnxr1EIrHdNHiM+//TXD2tHDnT19lh/gu6CvVa89Ff60a94YfeLto5+b/V9hzkKvFFMe8S3cQLLo/Zc5Dr3nXVqO6l3XJ8W51/Zx6q/rBIf/IP5wVH8o/l1qG9o/bG+UNSFpeOQX4+RqnAd8vUd69pBqejdVXW3ijOa+QKGDH9UE8ixa5/YUiUwDuEuzPrgdlEjO5njpCX/ZBGA9j4w7pbTG2FvnBXfVB3fmIqOdPaSgsVl4/L+0/wCI3GqEMFDp3TrBbrpQL5xXxZr5fcAjrzcO9f6RQaawwVNzSlp5Sdewr6vCxhsDDtO41PF1HYbL16RlFeqm6Smbu9IESebIgkLKIMY8Sg05Qyo0YJWyzW8lI1i0MPieJH9xXzhDj0msatly+CtslcXa5TNZqRJ1NNcic/n6qrqzMajQEBTu4RIHJlWSxWvpghcGH6hkAiCUaJenbBWSHabNu2rbq6eu7cuXgHgiLUiBeEh1JIeEiuEIIgsdjJj53bGyRXqNPplEoUu6S0B0iukMlktvQYiDSQXKHZbH7u7hZEgeQK2Ww2BGHUQQ4vSK7QaDTqdKgPWIAvJFcIQZCrqyveUaALyRW2lJ5DJkiusCNAcoVUo4LwUI0KwsNisZCMfRJDcoUmk0mv1+MdBbqQXGFHgOQKORyOSIT1SMsYQ3KFBoNBpVLhHQW6kFxhR4DkCiEIcnHBKCUXL0iuUKfTIR3tSQzJFXYESK6QOpASHupASkEASK6QSkIkPFQSIgUBILlC6pEv4aEe+RIeLpdLPakgNnq9nnpSQdHeIblCFotFJeQTG5PJRCXkExvqNjfhoW5zEx6qFhIeqhYSHjabzeejNTR9O4GcQweNGjXKarVaLBadTgfDsEgkQiapOHToEN6hOR8cxgzDgPDw8MLCQttLtVoNAIiPj8c1KLQg54E0MzPTze0/g9KLxeK0tDT8IkIRcirs3r17dHR08yUhISH9+vXDLyIUIadCAMD06dNtvexJXAXJrDAiIiI2Nhb5Pzg4eMCAAXhHhBakVQgAmDJliqurq1gszsjIwDsWFHHaFalRb5FWG/S6djR6Og8EJ0aPksvlge4Jd1Gbk/w5YHNoEh8OJHDOjIbOaRfm/1Z777rGJ4QHSNjIdD5siP7glsY/DBqc5sXiOHogdFQhbLbuWVvdJUEcHNnRZ0h7VuoqdSV5DePe8uPyHaqOjircvaYqsq+bbwimU+OSBrXCdHRT9bQvgxwpxKFa/Pc1tdidTfl7bgQurPA40bUih27EO6RQWmPkQGjNMtxB4IuZdRUOpUk6pFCvgcWSFmeWp2gLYne20eDQZbxDCk0GC2yhrkEdwgIDvdqhGY/J3LTvIFAKCQ+lkPBQCgkPpZDwUAoJD6WQ8FAKCQ+lkPBQCgkPpZDwtHeF36/+Zuz4IbaX02dMXLjoE+zD+HrJ/CnTxrW+TuGpggGD4isrK7AK6hHtXSHFU6EUEh4c+lTkHd6/Z+/2ysoKgUDYp3e/GZlv8vmCX3/7+cSJo/UNdRKJ+5CUl6dNnclgPP/D5BGj+s+Z/eHxk0f//POiQCAcPGhYdHRszqb1VVWVwUGh7733aZfO3ZA18/P/yN2WU1NTJZG4v5w6Jj1tOp3+6Gd94mT+5l9/qqt7GNQpBOlVg6DX6zf+su74iSNGoyHAv9PEia8OHDCkhUCwAGuFmzZv2Pzrz/2TB08Yly5XyC5ePM9ksRgMxuXLJb379PP18S8vv7UlN1soFE2c4FDy54pVi9+c9f60qTN37Ph1567cEyePfvDeZ1wIyvp+2Vdfffzr5j1MJvPo0UPLli8YNGjojMw3b9woy875EQDwasYMAEDB8SOLl8yPjYmfOCGjtrZm67ZNfn4BAACLxfLZ/Pdqa2vS06a7uLiVll5a9PWner0uddgo531JzwamChsa6rfkZqekpH46byGyZPKkKcg/P6zbTKPRkP9rHladPnPCQYXDho4cNXI8AGDmzHdOnT6enpbZu/eLAID0V6Yv/ebLmpqqgIBOG7PXRUXFzP/0awBAvxcHNjWptu/YPG7sKwwGY+2676KjY79dvg45GFRXPyj/+zYA4PSZE9fK/tyWe9Dd3QMAMHjQUJ1Ou3vPto6i8PKVEhiGR40Y/+Rbcrns199+vnipuKlJBQAQChxNaeRwHs3zw2axkb6iyEsPTy8AgFKpoNFoUmnDpImv2jZJSOidd3h/VXWlSqVUKhXjx6XZDub0f/4pLi4ym81pGSNtW8EwzOcLHIzWETBVKJM1AgA8PLyeXP76G+kQxMucPsvX1z87+4cHVffRDkatUQMAXFz+7cMmFIoAANKGeoVSDgDw9vZ9ciu5vFEicV/53frmCxlMPLtpYrpvgUAIAJDJGz09/2PxwMHdcrls3ZpNXl7eAABPT28MFHp6PKqOtiVyucwmEgCgUNiZu1IoFCkUci8vn/YzwCKmjYrYmHgAQF7ePtsSs9kMAFCpFC4urog/AIBSpbAlKLNYbJ1Oi6yGHBWRI63jSCTu3l4+Fy6ctS05daqAy+WGhXUJDe1Mp9MLjh9+cqu4uF4wDB84uMu2xDY0EXLEVqmwHsEW01oYENBp+MtjDh7ao1IpExJ6K5WKgwd3r1y5ISYmfu++37NzfoyI6HHmzImSkrMWi0WpVIjFLuFhXfR6/YKFH8964z0/X/+wsC55h/ev+2Hl6/83h8ViORjPtKkzly1f8O13ixISel+5cqHobOHUKa9DEARB0LChI//I22c0GHr16tPYKC0pKXJ1lQAAUganHjy0Z/2G7x/W1nQO71pefrvo7MlN2bu4XG5wSBidTl/1/dIP3vuse/coJ31nT4exYMGC5974bpmGJ2K5eT/DISXxhb5sNvv8+dMnTuZXV1UmJPSOjYnv3i3SarXs27/zzOnjvn4Bcz/4vKzsT51OGxMTHxwcqtfrLl48361LRGBgUPduUTU1VUVFJ0ePnmS7QnmSbds3hYd3TYhPBADodNrfd27p06df5/CuAIDa2pqj+YeGDR3p5eUdFtbZ1dXtxMn8w0cOKOSytLTpGemZyIVxz54vaDTqs+dOXbx4jkajCYUinU43ZvQkBoPRPzlFrVYVFh47feaERqseNnRUVFQMnU4XCoQ+3r5X/rzI5wuio2Pb+IVolObae9ruic8/4qZDfSoKttZJ/KCwGJIP+Ikq9ZX60hPSce/4P3cJRB3xori4aPHS+XbfWrs6p1OnYMwjwg2iKoyJif9pw1a7b3m4e2IeDp4QVSGXy/Wx127rgFBPKggPpZDwUAoJD6WQ8FAKCQ+lkPBQCgkPpZDwUAoJD6WQ8DikkC9i0uk05wXTMbGKPRwa+MUhhQIXZl0lyafTQZv6Kj2X75AFhzYO6AJplWZHSqBQ1huDujs0AppDCl082KE9+Kd21jpSSEemJK9BJGH6hzuk0Anjkd661FR6WhkWK/Tw5bKpIdnaAGyyNFTrH97VSnzYvV5ya8MWreGcIWXrH+jLzqpUjSal1OR4aU4Ehs1WK2Dimuf5JG4+HC6P3jmOH9TdCTnE5Jwtxsa2bduqq6vnzp2LdyAoQrULCQ+lkPCQXCEEQbYJR8gKyRXqdDq53E7XCDJBcoVcLlckInmmMskV6vV6lco53WjaLSRXSM3lS3iouXwJD4fDoc6FxMZgMFDnQor2DskVUo0KwkM1KigIAMkVMhiM9vaw0OmQXCEMw7YBT8gKyRUymcxWBsYgByRXaDabjUYj3lGgC8kVdgRIrpDNZvN4JJ+nluQKjUajVqvFOwp0IbnCjgDJFVI32AgPdYONggCQXCGVhEh4qCRECgJAcoXUFSnhoa5ICQ+DwWg/sxGgBMkVwjBsMBjwjgJdSK6wI0ByhRAEicVivKNAF5Ir1Ol0SiXW07dgDMkVUt1iCA/VLYbwUOdCwkOdCwlPRzgXknPooLS0NCaTaTKZFAqFxWLx8vIymUxGo3H37t14h+Z8yJmszuVyr169apvfubGxEQAQHEzOubjIeSCdNm0aBEHNl3A4nPT0dPwiQhFyKuzXr19ERETzJX5+fqNHj8YvIhQhp0IAwJQpU4TCRzOrs9nsyZMn4x0RWpBWYVJSUpcuXZD//f39x44di3dEaEFahQCAjIwMkUjEZrMnTpyIdywo0qYrUrPJolNb0A/GyfSISIzoEi+Xy18aNLpJTrxehmwunQM9vY49pV34vwuqa2eUslojJKBGbMYaJpsOmyxRfcVxA1vLo2xN4YV8mbTGFJPsJnRzdAZ5iuejSW66fVlhNlgGp3m1tE6LCkuOyFSN5sThHWtq4/ZJWZFMozClpNu3aP9QK683SqsNlL92QlRfN0CjPbhtv5edfYXSaoPVSs3k045gceh1lfbzuOwrVCthjwAuylFRPAPufly9Brb7lv1GhclgMelRDoriWTCbrBqVfYVkbtp3ECiFhIdSSHgohYSHUkh4KIWEh1JIeCiFhIdSSHgohYSHUkh4cFZoNpszpoz5cX0W8hKG4bKyUnxDIhw4K6TRaEKhiMt99FTk2xWLVmYtwTckwoFbQr7VaqXRaAwG48d1m20LjWQc2gD5pOiV7xyFH3/ydlVVZe5v+5CXW3Kzg4NCk5KSkZdTp4/v1i1y3kcLps+YGBwUGhQUumfvdoNBv3Z1zmuvvwIAyEjPnJH55rLlC04WHgMADBgUDwDYmnvAx9sXALD/wK7fd26RSuu9vX0HDRw6aeKrrY9DMv+LDwIDgvQGfX7+IavVGhfba9zYV7bk/vLX9aturpLp095ISUlF1nxYW/PDDysvXylhszmdw7tmZr7ZtUv3Zyrhxv/+Wr8h69atG1wu1Kd3v1mz3hMJRQCAxz7ppIlTtm7L2fn7EbHoUWfHxUs/v3H9Wu6W/Y5/+c45kPZPHlxTU3Xv3t/IyyNHDx7K24v8f/dueWVlRf9+g5GXFy+ev3nr+pKvVy1auMLPL2DRwu9s80hkpGXGxSb4ePuuztq4OmujxM0dALBp808//bx64IAhH879on/y4B2//7pi1eKnxrNt+2YAwMoVGyZNnFJ0tvDDj2cnJfVftfKnsLAuy5YvqKysAAA0NkrnvJ2palK+NXvuzNffNplM77z7mu0jtKWEioq7H8x9w2QyffThl1Nf/b+iopNfffWxLYbmn3TE8LEwDJ88mY+8ZTKZiovPDBz4klO+fOfUwqSk/sxVS86eOxUcHHr16pXq6gcPH1bX1dV6eXmfOl0g4At69nwBWZPBZH7+2RJbn5W+Sf1tBxl//0Cx2EUmb4yKikGWSKUNuVuz53+2OLnfIGSJROKxKmvpW7PnIj/2lujUKfjttz4EAHQO75p3eF/XLhFjRk8EAMx+84MzRSdLr14ODAz6bctGVxe3Fd/+iPyGUganZkwZfShv75zZc9tYwpbcX+h0+vJv1goFQgCAUChasuyLq1ev9OgR9+QnTUjofTT/0OhREwAAly4Vq9XqQQOHOuXLd45CkVAUF5tw9mxhRnrm4aMHYnr0lMkbDx85MG3q64WnCpL69mexHqUxdusW+Vifo1a4fLnEbDYvXjJ/8ZL5yBIk307aUN+6Qg773yMtm81h/rN3T08vAIBSqQAAlJScrW+oSx3+om1Nk8nUUF/X9hJKr16OjU1A/CGSAAC3bt9AFD72SYe+NOKrhfMqKysCA4MKTxeEhoYHBYW08XtoHaddziQnD/72u0WVlRWnThV89OGXskbp77u2vNh3QGVlxayZ79pWg7ht9QcAaJRJAQBLFmd5evwn/87X1//5gkRqPPI7kMkbe/d+8fXX5jRfgc8XtL0EjUbtIv43SVcoFCFHDuTlY580qU+ySCQ+mn9o2tSZ586eSkub/nwf4UmcpjApqf/KVUuWfvMlBPFe7DtAp9f9/MvalVlLmh9F20LzvFbhP1UtMDDIWXE2L1ypVDhSsru7p0r1b0d+uVwGABD8Uykfg8ViDR48LP/YH927Rak16oEDnHMidGa7UCwSx8Um3Lx5PXXYKCaTKRQIB/QfcuNGWfOj6FPhciGZrNFiedR/IzY2gUaj7d23w7aCTqdzVsBxcb3++uvqrdv/e+7CIyKiS69e1usfJYqdPn0cAGA7kT/J0JdGSKUNP6xfFRUV4+Xl7UDs/8GZTfvk5ME0Gm34y4+6gY0cOR4AYLsWbQs9ouOamlQrVy05evTQuXOn/f0Cxo6ZfO7c6U/nv5d3eP9vW37JmDL69p2bTol26pTXhULRhx/N3pKb/Ufevi8XfLR46fxnKiEjLVOv1338yZyC40e2btu04efVsTHxMT16trR+eFiXwMCgmpoqZ13IIDizad83qX9xcZG3tw/yslvXiLjYhGc6iqakpN66fSP/2B/ni88MfWlEnz79Zr/5vqen1969Oy5ePC+RuL/Yd4CHu3NyzP18/deuzv5xQ1bu1mwajRYe3nXM6EnPVIK/f+DyZWt/2rhm+bdfQRAvZXDqGzPfbb0V371bVE1NVf/kZ/hZPxX7fSouHJUZ9aBHfzcn7okCAPD5F3PNsHnp4qxn3fDva011FdqXXrXTrYKQI14UFxe1dNBbuzqnU6f2OLLFsYLDBccPX7x4fsV3Pzq3ZEIqjImJ/2nDVrtvOesw63QOH95vMpu+WbYmNibeuSUTUiGXy0VunxKIlSvWo1Qy9ciX8FAKCQ+lkPBQCgkPpZDwUAoJD6WQ8FAKCQ+lkPBQCgmP/RtsbC7NAqhxZ9oRDCaNL7I/DJ79Wih0ZTXcd9rzcQrHkVbpecJnUegZwEEz/5jimTEZYJ9g+4M5tVgL/cK4p3fXohwYRZu4dEzKgeg+wfaT/1obzPL6eeWdUnWPZImrF5vBpC58sMZqtTY+NNy5rBS6MHsPl7S02lOGlL13XVN6SlF7T89gEvLAarFaAbDSaYT8/XEgBpdPj+4r7vZCa3nPbZ0txqAj3sDOAIDdu3fX1NTMmTOnDeu2O9hceluuSNr61L4tg0S3Q2gMM6CbCBp8GyHzZ+sgkFwhh8MRiVo7kZAAkis0GAwqlQrvKNCF5AohCHJ1bW2GABJAcoU6nU4ul+MdBbqQXCGPx6NqIbHRarVULSQ2TCaTzWbjHQW6kFyh2Ww2Go14R4EuJFfYESC5Qh6PR/pJ0UmuUKvVKhQKvKNAF5Ir7AiQXCGHw7FNykxWSK7QYDA0NTXhHQW6kFxhR4DkCplMZusjX5IAkis0m80GMg5T2xySK7QNfEdiyK+wjfldxIX8CkkPyRUyGAzqcobYwDBMXc5QtHdIrpBKQiQ8VBIiBQEguUIqj5TwUHmkFASA5AoZDAaVhEhsYBimkhCJDXU5Q3ioyxnCw2azeTwe3lGgC8kVGo1GrVaLdxToQnKFVC0kPFQtJDwQBFF9KoiNTqcjfZ+Kto7+RCwyMjKuX7/OYDCQOeWRv/7+/vv27cM7NOdDzlqYlpaGPOlFMhBpNBqDwRg5ciTecaECORWmpqYGBgY2XxIUFDR+/Hj8IkIRcipEKqKtOUGn04cMGULWDAzSKhw2bJitIgYHB5O1CpJZIQAgPT2dz+czGIyUlBSxWIx3OGhBzitSG+np6Xq9Picnh6xH0XakUNFg/Puq5uF9g1pu1mlgSMhU1DshhdcCw1YAGAz7Y8s/E0I3tkFjhgQMSMD0DuKE9eC7+7aLPHH8FV45qbh2Rmk2WfkSHs+Fy2QzmGwGk+OEL93pwEbYbIRNBtigNqqlGqvFGtlb9MIwnOcdx1Nh2VnVuUNSV1+hyFvAFRAvPcKoMzXVa2tvyxKGSnoNwe3BMj4KTUaw94cak5nuFe7GZLfHCtd2rBZr3R2ZxWwa86YvxMOhLyMOCg06ePOi+74RngI3+xMvEBGDxnTnXNUrHwVIvLE+QWKtUK+Fd2bV+HT3ap9nOwe5f7lm1ExvFw8WljvFul2Y82WFX5Q3Kf0BADr19N2x4oFODWO5U0wVbvv2Qac4bzqpJ54JSfTbsrQSyz1i921eOCpjC3k8F/tzR5EGFofpEepWsK0esz1ipBA2Wy/lyySdSP4AHcHFR3D/pk5ej1EKMkYKT++VenXGuQmMJR4hrqd2S7HZFxYKLbClvLRJEtgebzSXXNo/9/MXVConf90iT75SBisasKiIWCisuKGFRCQ/BT4JR8CpuK7BYEdYKLxTquFLSJ7M+SQCCe9OKRb5j22dOc0RVDKzSyBaFzLnLuw+dXarUlXv5uobGz2kf1IGi8U5fW5baVlBvz6vHC74salJ6ufbdcKoTzw9gpBNqmtu7ctb+aD6hkjo7iEJfNoenhOBBFJUyS0WK52O7l03LGphfaWOgc6N0PwTP/9xdG1MVMrE0fOjIwYVntmya/9S5K3Kqr9Onc2dMOrTqa8sVyjrtu9ZiCyva6j4MXuWStWQmvJmcp+06oe30AgMQacyY9DMR70W6jUwk0VH45eoVDUcP70pffyi6MiByBKx0H33wW9Gpb6PvJye/p1IKAEA9E2cePDI9xqtks8T/3F0DY1GnzPzFwHfFQBAo9P3HFzu9NgQWFyGRmXmi9D9klFXqFGZXbxRuZa58/cFGDbn7void9cX/yyzAgCUTY+a1Rz2o9vori4+AACVqoHF5NwqL+6dMA7xBwBg0FH8BvhuHF0T8Wshl8dQNRi8uji/ZFWTFAAwI2Oli9iz+XKJm/+dvy82X8JksAAAFgusapLCsNnN1cf50dhDqzCyuaifqlBXyBMxDFpUfokQ9Cgdxnad8lSQyqdWY9Rp1GSA0T6KYnE5Q6PRuHyG2eB8i+Eh8TQarajkd9sSg1HX+iZcLt9dEnD1+nGz2eT0eJ7EqDPzxcRXCACQ+HJ0KucPR+guCeibOOnGzTPZWz4MPFn+AAAClklEQVQouXygoDB72apxVTU3W99qyIDXGmVVa3567WzxznMXdheezXV6YAh6tVHgwsJgMnks2oXhMfyyEq3Qw/mt+5HD3nURexYV77xVXiwSukd27y8Weba+SVyPoTpdU+HZ3EP5a7w8QjoFRDZI7zs9MABAU4M2NJqPRsmPgcVTe43KnLvsQecX0WpEt0/uX6kZku7uE4R6cgkWtZAvYnoHc9UyXSvJMvMXD7K7vFNA1P0HZXbKhMSfvL/HiUGu2zjzYV35k8v9fbpWPbR/cP76s+MtlWbQmjhcGgb+sMudaag2HNpYF9zLr6UVZPIa+29YaYBmJ0Iaje7q4u3ECJWqBhi2c42DdE+0u4mbq29LpVWX1SUOFYVGC5wYYUtgUQsBAB5+HE9/tuKh2sXH/qdq5evABrHIw1lFaRV6Og3Gxh+miRcvTfFsvE/yUXwQGu/Jhk71wmx32ClksugjXvOuuFiN2R5xofqvusRUF1dP7JLTMU0m8wzgJo+VVJXVYblTLKm50RDdRxAeg+l0e1jnAwZH8vuOEFdcImFdrCqri0iAopKw7gWHT5+K2vv6gz8/9AyTiL2waPyijVqmU1QpEoe6hPXA6BKmObj1bDKZLHnZdfJ6k3uoROBK1MwaXZNRelfG4ViHvOrp4o5P5yyc+xfW3defz5NLawwCCU/gweOJOXRGe8/1tlisepVB1aDVNGpdPVnxg1wCu+KZGYR/F1EAgLLRdLdMc+dPtVJqhE1WNsQUunP1aiweJrQdNo+pkRtMethsskh8OCFR/NBovsQH/46+7UKhDavVatRbNCpYr4GtFryjeQwajcuj8URMiN+++vS0L4UUz0F7P/FQPBVKIeGhFBIeSiHhoRQSHkoh4fl/bPHmHwjvxIIAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"Create or update a user profile memory based on the user's chat history. \n",
    "This will be saved for long-term memory. If there is an existing memory, simply update it. \n",
    "Here is the existing memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "        \n",
    "    # Format the existing memory in the instruction\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "\n",
    "    # PAY ATTENTION: here is where we define the new_memory variable.\n",
    "    # Invoke the model to produce structured output that matches the schema\n",
    "    new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # Overwrite the existing use profile memory\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, new_memory)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61942b29-1a34-4ae9-ba54-5d3c24c6f211",
   "metadata": {},
   "source": [
    "## This code is very similar to the one we used in the last exercise\n",
    "\n",
    "The previous code creates a **chatbot with memory** using LangGraph, OpenAI's GPT, and structured memory storage. It ensures the chatbot can **remember user information** and **update its memory** based on new conversations. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Import Required Libraries\n",
    "```python\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "```\n",
    "- Libraries for **memory management**, **graph-based workflows**, and **message handling** are imported.  \n",
    "- `Image` and `display` are used to **visualize the workflow graph** later.\n",
    "\n",
    "#### System Messages and Instructions\n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory... {memory}\"\"\"\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"Create or update a user profile memory... {memory}\"\"\"\n",
    "```\n",
    "- These define **system prompts** for the AI.  \n",
    "- **`MODEL_SYSTEM_MESSAGE`**: Guides the chatbot to **use memory** (if available) to **personalize responses**.  \n",
    "- **`CREATE_MEMORY_INSTRUCTION`**: Guides the chatbot to **update memory** after each interaction.\n",
    "\n",
    "#### Chatbot Response Function\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "This function generates **responses** for the chatbot by:\n",
    "\n",
    "1. **Fetching Memory:**\n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = store.get(namespace, \"user_memory\")\n",
    "```\n",
    "- Retrieves memory for the user (if it exists) based on their **user ID**.\n",
    "\n",
    "2. **Formatting Memory:**\n",
    "```python\n",
    "formatted_memory = (\n",
    "    f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "    f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    ")\n",
    "```\n",
    "- Prepares memory (e.g., name and interests) as a **text summary** for the AI.\n",
    "\n",
    "3. **Generating Response:**\n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "```\n",
    "- Combines memory with chat history and **invokes the AI model** to generate a personalized response.\n",
    "\n",
    "#### Memory Update Function\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "This function **updates the memory** after processing a user's message:\n",
    "\n",
    "1. **Fetch Existing Memory:** Same as before—loads memory from the store.  \n",
    "2. **Format Memory Instruction:**  \n",
    "```python\n",
    "system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "```\n",
    "- Prepares instructions for the AI to **update memory** based on the latest chat.  \n",
    "3. **Invoke Model for Memory Update:**\n",
    "```python\n",
    "new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "```\n",
    "- AI updates the memory (e.g., adds new interests).  \n",
    "4. **Save Updated Memory:**\n",
    "```python\n",
    "store.put(namespace, key, new_memory)\n",
    "```\n",
    "- Overwrites the **user memory** in the store.\n",
    "\n",
    "#### Workflow Graph Definition\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- Defines a **graph-based workflow** to control the chatbot's process step-by-step.\n",
    "\n",
    "**Nodes and Edges:**\n",
    "```python\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "```\n",
    "- **Nodes**: Steps in the process (response generation → memory update).  \n",
    "- **Edges**: Defines the **sequence** of steps.  \n",
    "  - Start → Respond → Update Memory → End.\n",
    "\n",
    "#### Memory Setup\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- **`across_thread_memory`**: Long-term memory (persists between sessions).  \n",
    "- **`within_thread_memory`**: Short-term memory (temporary during the conversation).\n",
    "\n",
    "#### Compile and Visualize Workflow\n",
    "```python\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- **Compiles** the graph with memory systems attached.  \n",
    "- **Visualizes** the workflow as a **diagram** to show how steps connect.\n",
    "\n",
    "#### Example Scenario\n",
    "1. **User Input:** \"My name is Julio, and I like AI.\"  \n",
    "2. **Response:** \"Hi Julio! AI is fascinating. What else are you interested in?\"  \n",
    "3. **Memory Update:** Saves `\"Julio\"` as the name and adds `\"AI\"` to interests.  \n",
    "4. **Next Interaction:** \"Tell me about robots.\"  \n",
    "5. **Response:** \"Since you like AI, you might also enjoy learning about robots!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df0392-10e2-42c6-935d-2738c5a6c5bf",
   "metadata": {},
   "source": [
    "## Let's try this new chatbot and see how it saves the user information in the User Profile memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe5af33-7f7c-42a0-a360-efb5d982006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! That sounds like a lot of fun. San Francisco is a great city to explore on a Vespa, and Whole Foods has some delicious options. Do you have any favorite spots or foods you like to get there?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cb8f05-d5b0-4a51-a818-0c7286e9c790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio',\n",
       " 'interests': ['driving Vespa',\n",
       "  'exploring San Francisco',\n",
       "  'eating at Whole Foods']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234ae29-7833-4b58-adfa-5296e4577b05",
   "metadata": {},
   "source": [
    "## Let's review what we just did\n",
    "\n",
    "The previous code demonstrates how to **run the chatbot with memory** and **inspect the updated user memory** after processing input. Here’s a simple explanation step-by-step:\n",
    "\n",
    "#### Configure Memory Settings\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`:** Tracks **short-term memory** for the current session.  \n",
    "- **`user_id`:** Tracks **long-term memory** that persists across sessions.  \n",
    "- These IDs are used to **organize memory** by user and session.\n",
    "\n",
    "#### Provide User Input\n",
    "```python\n",
    "input_messages = [HumanMessage(\n",
    "    content=\"Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\"\n",
    ")]\n",
    "```\n",
    "- **Simulates a user message**: Julio introduces himself and shares hobbies and preferences.  \n",
    "- **`HumanMessage`:** Marks the input as a message from the user.\n",
    "\n",
    "#### Process the Input Using the Graph\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Streams output step-by-step** through the **graph workflow** (defined earlier).  \n",
    "- **Input Processing:**\n",
    "  1. **Call Model**: Uses memory to personalize a response.  \n",
    "  2. **Write Memory**: Updates long-term memory with new details.  \n",
    "- **Print the Response:**\n",
    "  - Shows the **AI's reply** to the user after analyzing the input.  \n",
    "  - **`pretty_print()`**: Makes the output easier to read.\n",
    "\n",
    "#### Retrieve Updated Memory\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value\n",
    "```\n",
    "- **Access Saved Memory:**\n",
    "  - **`across_thread_memory`:** Stores long-term memory across sessions.  \n",
    "  - **`namespace`:** Fetches memory tied to the **user ID = \"1\"**.  \n",
    "  - **`get`:** Retrieves the stored data under the key `\"user_memory\"`.  \n",
    "\n",
    "- **Inspect Memory:**\n",
    "  - Outputs the updated profile stored in memory based on the user’s input.  \n",
    "\n",
    "#### Example Output: Memory Content**\n",
    "```python\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"vespa\", \"San Francisco\", \"Whole Foods\"]\n",
    "}\n",
    "```\n",
    "- **Memory Update:**\n",
    "  - The AI has:\n",
    "    1. Extracted the **name** (\"Julio\").  \n",
    "    2. Added **interests**: \"vespa\", \"San Francisco\", and \"Whole Foods\".  \n",
    "\n",
    "#### Key Features\n",
    "1. **Personalization:** The chatbot uses stored memory to give **context-aware responses**.  \n",
    "2. **Learning from Conversations:** Updates memory with new information as the chat progresses.  \n",
    "3. **Memory Persistence:** Long-term memory ensures the chatbot **remembers details** even after restarting.  \n",
    "4. **Streaming Output:** Responses are generated and displayed **in real time**, chunk by chunk.\n",
    "\n",
    "#### Example Interaction\n",
    "1. **User Input:**  \n",
    "   `\"Hi, my name is Julio and I like to drive my vespa around San Francisco and eat at Whole Foods.\"`\n",
    "\n",
    "2. **Chatbot Response:**  \n",
    "   `\"Hi Julio! It's great that you enjoy driving your vespa around San Francisco. Whole Foods is a fantastic place to eat!\"`\n",
    "\n",
    "3. **Memory Check:**  \n",
    "   **Before Chat:**  \n",
    "   ```\n",
    "   {}\n",
    "   ```\n",
    "   **After Chat:**  \n",
    "   ```\n",
    "   {\n",
    "       \"user_name\": \"Julio\",\n",
    "       \"interests\": [\"vespa\", \"San Francisco\", \"Whole Foods\"]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "#### Final Thoughts\n",
    "This setup creates a **smart chatbot** that can **learn and remember user preferences** over time. It processes user input, **updates its memory**, and **retrieves the stored information** when needed—making conversations feel **personalized and consistent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5587651-f790-4c30-9d99-4ef4c8139b4b",
   "metadata": {},
   "source": [
    "## That worked well. Let's how the previous app works if we define a more complex Memory Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459eb499-b9cf-4d87-8af9-0fb58425c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    preferred_encoding: Optional[List[OutputFormat]] = None\n",
    "    favorite_telegram_operators: Optional[List[OutputFormat]] = None\n",
    "    preferred_telegram_paper: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    preferred_flag_color: Optional[List[OutputFormat]] = None\n",
    "    semaphore_skill_level: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None\n",
    "    trust_level: Optional[List[OutputFormat]] = None\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "\n",
    "# PAY ATTENTION HERE: this is the complex schema we will use\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3787cd-2bc1-4a63-bcc6-329a160a949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for TelegramAndTrustFallPreferences\n",
      "pertinent_user_preferences.communication_preferences.semaphore\n",
      "  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "\n",
    "# Conversation\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# PAY ATTENTION: do not get distracted by the <convo> tag here. We will explain it below.\n",
    "# Invoke the model\n",
    "try:\n",
    "    model_with_structure.invoke(f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\")\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551637b1-1e81-4697-8b7f-ea9c756bed1b",
   "metadata": {},
   "source": [
    "## Oooops! That did not work so well. What happened?\n",
    "\n",
    "The previous code is trying to **extract structured preferences** about communication methods and trust fall preferences from a given conversation. It uses **Pydantic models** to define the structure of the data, and then **binds that structure** to an AI model to ensure the output follows the expected format. However, it fails due to a **validation error**. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Defining Data Models (Schemas)\n",
    "\n",
    "**Basic Data Model: OutputFormat**\n",
    "```python\n",
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "```\n",
    "- This defines the **structure of preferences** with two required fields:\n",
    "  - **`preference`**: Describes the user's preference.  \n",
    "  - **`sentence_preference_revealed`**: Stores the sentence where the preference was mentioned.  \n",
    "\n",
    "**Communication Preferences**\n",
    "- **TelegramPreferences**, **MorseCode**, and **Semaphore** models define preferences for **different communication methods** (telegram, morse code, semaphore).  \n",
    "- Each model includes **optional lists of OutputFormat objects**.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None\n",
    "```\n",
    "- Stores preferences for **morse code key types** and **abbreviations**.  \n",
    "- These fields are **optional** (can be missing or `None`).\n",
    "\n",
    "**Trust Fall Preferences**\n",
    "```python\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None\n",
    "    trust_level: Optional[List[OutputFormat]] = None\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None\n",
    "```\n",
    "- Defines preferences for **trust fall activities**, like:\n",
    "  - Fall height.  \n",
    "  - Trust level.  \n",
    "  - Catching technique.  \n",
    "\n",
    "**Nested Structures:**\n",
    "```python\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "```\n",
    "- Combines all communication methods under a single model.\n",
    "\n",
    "```python\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "```\n",
    "- Combines **communication and trust fall preferences**.\n",
    "\n",
    "```python\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences\n",
    "```\n",
    "- The **final model** wraps everything into one structure for processing.\n",
    "\n",
    "#### Binding Model with Schema\n",
    "\n",
    "```python\n",
    "model_with_structure = model.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "```\n",
    "- Forces the AI output to match the **`TelegramAndTrustFallPreferences`** structure defined above.  \n",
    "- Ensures the AI response adheres to the **nested schema**.\n",
    "\n",
    "#### Error Scenario\n",
    "\n",
    "**Conversation Input:**\n",
    "```python\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "...\"\"\"\n",
    "```\n",
    "- The conversation contains preferences for:\n",
    "  - Morse code key: **\"straight key\"**.  \n",
    "  - Fall height: **\"higher fall\"**.  \n",
    "  - Catching technique: **\"diamond formation\"**.  \n",
    "  - Paper type: **\"Daredevil paper\"**.  \n",
    "\n",
    "**Error When Invoking Model:**\n",
    "```python\n",
    "1 validation error for TelegramAndTrustFallPreferences\n",
    "pertinent_user_preferences.communication_preferences.semaphore\n",
    "  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]\n",
    "```\n",
    "\n",
    "**What Happened?**\n",
    "- The AI model **did not generate data** for the **`semaphore` field** in `communication_preferences`.  \n",
    "- Instead, it returned **`None`**, but the model **requires a valid object** of type **`Semaphore`**, even if all fields are optional.\n",
    "\n",
    "\n",
    "**Why Did It Happen?**\n",
    "- The **schema expects the `semaphore` field** to exist in the output as a **dictionary or object**, even if it contains **no data** (empty fields).  \n",
    "- Instead, the AI completely **skipped** this section, leaving it as **`None`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bebd58-416a-442f-9cc9-6122ed6c6f2b",
   "metadata": {},
   "source": [
    "## What is the <convo> tag doing in the previous code?\n",
    "In the previous code, the `<convo>` tag is used to wrap the conversation text before passing it to the model. However, **it is not a predefined LangGraph or Pydantic feature—it's simply a developer-defined way of structuring the input text.**\n",
    "\n",
    "#### What does `<convo>` do in this code?\n",
    "1. **Encapsulation**: It acts as a wrapper around the conversation to indicate where the dialogue is located.\n",
    "2. **Contextual Cue for the Model**: The `<convo>` tag helps the model recognize that the text inside is a conversation from which it should extract structured preferences.\n",
    "3. **No Special Processing by LangGraph**: LangGraph itself does not inherently process `<convo>`—it's just part of the prompt formatting.\n",
    "4. **Potential XML-Like Usage**: Depending on how the underlying language model is trained, it might recognize `<convo>` as a way to highlight the conversation structure, making it easier to extract relevant data.\n",
    "\n",
    "#### Why use `<convo>`?\n",
    "- It visually separates the conversation from the instruction (`Extract the preferences from the following conversation:`).\n",
    "- If the model has been trained to recognize XML-like structures, it might improve parsing accuracy.\n",
    "- It prevents confusion when processing multi-line text in an LLM pipeline.\n",
    "\n",
    "#### What happens when the model is invoked?\n",
    "1. The **conversation text** (wrapped in `<convo>` tags) is sent to `model_with_structure.invoke(...)`.\n",
    "2. The **structured model (`TelegramAndTrustFallPreferences`)** expects extracted user preferences from the conversation.\n",
    "3. The **LLM extracts preferences** based on the conversation content and maps them to the structured schema.\n",
    "4. If the extraction doesn't conform to the expected schema, a **`ValidationError`** is raised.\n",
    "\n",
    "#### Example of Expected Extraction:\n",
    "From the given conversation, the model might extract:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"pertinent_user_preferences\": {\n",
    "    \"communication_preferences\": {\n",
    "      \"telegram\": {\n",
    "        \"preferred_telegram_paper\": [\n",
    "          {\n",
    "            \"preference\": \"Daredevil\",\n",
    "            \"sentence_preference_revealed\": \"Shall I use our 'Daredevil' paper for this daring message?\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"morse_code\": {\n",
    "        \"preferred_key_type\": [\n",
    "          {\n",
    "            \"preference\": \"straight key\",\n",
    "            \"sentence_preference_revealed\": \"Morse, please. I love using a straight key.\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    \"trust_fall_preferences\": {\n",
    "      \"preferred_fall_height\": [\n",
    "        {\n",
    "          \"preference\": \"higher fall\",\n",
    "          \"sentence_preference_revealed\": \"Tell him I'm ready for a higher fall.\"\n",
    "        }\n",
    "      ],\n",
    "      \"preferred_catching_technique\": [\n",
    "        {\n",
    "          \"preference\": \"diamond formation\",\n",
    "          \"sentence_preference_revealed\": \"I prefer the diamond formation for catching.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Conclusion:\n",
    "- `<convo>` is **not** a special feature in LangGraph or Pydantic.\n",
    "- It is **a developer-defined wrapper** to structure the conversation inside the prompt.\n",
    "- It **helps the model extract preferences** from a well-defined conversation block.\n",
    "- The **Pydantic schema validates** the structured output extracted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4322f59-3693-4e65-b7f6-4dae18dd0ed9",
   "metadata": {},
   "source": [
    "## Solving these issues with TrustCall\n",
    "* In the previous code, we took the approach to regenerate the Profile schema from scratch each time we chose to save a new memory. This is inefficient, potentially wasting model tokens if the schema contains a lot of information to re-generate each time. We also may loose information when regenerating the profile from scratch.\n",
    "* In addition, complex schemas can be difficult to extract.\n",
    "* We can solve many of these issues using the open-source library [TrustCall](https://github.com/hinthornw/trustcall) developed by one of the members of the LangChain team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720d76b-fe76-4a3a-bf9e-57920b056e70",
   "metadata": {},
   "source": [
    "## First, let's see how TrustCall works with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d965a98-cd75-46a4-8855-5a2f185cfece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION: this is the conversation variable we will use below.\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"I am interested in Gen AI startups.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2109f58e-1630-4c9a-9fad-df8f356d67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION: here is where we import create_extractor from truscall\n",
    "from trustcall import create_extractor\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    interests: List[str] = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\"\n",
    ")\n",
    "\n",
    "# Instruction\n",
    "system_msg = \"Extract the user profile from the following conversation\"\n",
    "\n",
    "# PAY ATTENTION: See how we use the extractor and compare it with the previous approach.\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa957da-eca9-40cd-a4a0-1a4fd4eb3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_B0RGcNrAOMYuWsgw2oYn2Gta)\n",
      " Call ID: call_B0RGcNrAOMYuWsgw2oYn2Gta\n",
      "  Args:\n",
      "    user_name: Julio\n",
      "    interests: ['Gen AI startups']\n"
     ]
    }
   ],
   "source": [
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43242299-dbbf-4d19-a357-d84aec3b6438",
   "metadata": {},
   "source": [
    "## Let's see what we just did\n",
    "\n",
    "This example demonstrates how to use **TrustCall**, a library designed to **simplify schema extraction** from conversations, making it more **efficient** and **reliable** than regenerating profiles from scratch each time. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### The Problem with Previous Approaches\n",
    "- Earlier methods required regenerating the **entire profile schema** each time the chatbot updated the memory.  \n",
    "- **Issues:**\n",
    "  1. **Inefficient**: Wastes tokens by regenerating all data, even unchanged parts.  \n",
    "  2. **Risk of Data Loss**: Important information might be **overwritten** or lost.  \n",
    "  3. **Complex Schemas**: Extracting information from large, nested schemas is difficult and error-prone.  \n",
    "\n",
    "**Solution**: **TrustCall**—a tool to efficiently **update specific fields** in schemas without regenerating the entire structure.\n",
    "\n",
    "#### TrustCall Example\n",
    "\n",
    "**Step 1: Conversation Data**\n",
    "```python\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "    AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "    HumanMessage(content=\"I am interested in Gen AI startups.\")\n",
    "]\n",
    "```\n",
    "- **Simulated chat**:\n",
    "  - Julio introduces himself and mentions his interest in **Gen AI startups**.  \n",
    "- This data will be **processed to extract user details**.\n",
    "\n",
    "\n",
    "**Step 2: Define Schema**\n",
    "```python\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    interests: List[str] = Field(description=\"A list of the user's interests\")\n",
    "```\n",
    "- Defines the **data structure** for the user profile:\n",
    "  - **`user_name`:** Stores the user’s name (string).  \n",
    "  - **`interests`:** Stores a **list of interests** (multiple values allowed).  \n",
    "- Each field has a **description** to help the AI understand what data should go there.\n",
    "\n",
    "\n",
    "**Step 3: Initialize the Model**\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **GPT-4o** is used as the AI model.  \n",
    "- **`temperature=0`** ensures **consistent outputs**.\n",
    "\n",
    "**Step 4: Create the Extractor**\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],  # Schema for extraction\n",
    "    tool_choice=\"UserProfile\"  # Focus only on this schema\n",
    ")\n",
    "```\n",
    "- **TrustCall Extractor** is created to **analyze and extract data** based on the provided schema.  \n",
    "- **`tools`:** Specifies the schema to use (`UserProfile`).  \n",
    "- **`tool_choice`:** Ensures the AI uses **only this schema**, avoiding unrelated data.\n",
    "\n",
    "**Step 5: Define Instructions**\n",
    "```python\n",
    "system_msg = \"Extract the user profile from the following conversation\"\n",
    "```\n",
    "- Provides a **clear instruction** to extract **user information** from the chat.  \n",
    "- Keeps the AI **focused on schema extraction** rather than free-form responses.\n",
    "\n",
    "\n",
    "**Step 6: Invoke the Extractor**\n",
    "```python\n",
    "result = trustcall_extractor.invoke(\n",
    "    {\"messages\": [SystemMessage(content=system_msg)] + conversation}\n",
    ")\n",
    "```\n",
    "- **Processes the conversation** using the schema and instruction.  \n",
    "- **Extracts relevant fields** (e.g., name and interests) into the `UserProfile` structure.  \n",
    "- Automatically **fills only the relevant fields** in the schema without overwriting existing data.\n",
    "\n",
    "\n",
    "**Step 7: Display Results**\n",
    "```python\n",
    "for m in result[\"messages\"]: \n",
    "    m.pretty_print()\n",
    "```\n",
    "- Prints the **extracted data** in a structured and **readable format**.\n",
    "\n",
    "#### Example Output\n",
    "```\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"interests\": [\"Gen AI startups\"]\n",
    "}\n",
    "```\n",
    "- Extracted data:\n",
    "  - **`user_name`:** \"Julio\" (from the first user message).  \n",
    "  - **`interests`:** [\"Gen AI startups\"] (from the second user message).  \n",
    "\n",
    "#### Why is TrustCall Better?\n",
    "\n",
    "1. **Efficiency**:  \n",
    "   - Only **updates fields** that need changes, saving tokens and reducing API costs.  \n",
    "\n",
    "2. **Reliability**:  \n",
    "   - Preserves **existing data** and avoids accidental overwrites.  \n",
    "\n",
    "3. **Scalability**:  \n",
    "   - Handles **complex schemas** easily without manual parsing or restructuring.  \n",
    "\n",
    "4. **Flexibility**:  \n",
    "   - Works with **specific fields**, so even partial updates are possible.  \n",
    "\n",
    "5. **Ease of Use**:  \n",
    "   - Requires minimal configuration—just define the schema and let the extractor handle the rest.\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "This example shows how **TrustCall** can simplify **data extraction and memory updates** in AI workflows. Instead of **rebuilding the profile** each time, it **updates only the relevant parts**, making it faster, more reliable, and better suited for **complex schemas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7edb4-8db3-4ecd-911d-d32f84518917",
   "metadata": {},
   "source": [
    "## Now that we are more familiar with TrustCall, let's try it with our problematic use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f072e41a-8a9d-4f67-bc51-5d34e7f9038e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TelegramAndTrustFallPreferences(pertinent_user_preferences=UserPreferences(communication_preferences=CommunicationPreferences(telegram=TelegramPreferences(preferred_encoding=[OutputFormat(preference='Morse', sentence_preference_revealed='Morse code or standard encoding?')], favorite_telegram_operators=None, preferred_telegram_paper=[OutputFormat(preference='Daredevil', sentence_preference_revealed='Shall I use our \"Daredevil\" paper for this daring message?')]), morse_code=MorseCode(preferred_key_type=[OutputFormat(preference='straight key', sentence_preference_revealed='I love using a straight key.')], favorite_morse_abbreviations=None), semaphore=Semaphore(preferred_flag_color=None, semaphore_skill_level=None)), trust_fall_preferences=TrustFallPreferences(preferred_fall_height=[OutputFormat(preference='higher', sentence_preference_revealed=\"I'm ready for a higher fall.\")], trust_level=None, preferred_catching_technique=[OutputFormat(preference='diamond formation', sentence_preference_revealed='I prefer the diamond formation for catching.')])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PAY ATTENTION: here is where we create the extractor.\n",
    "# See that the variable name \"bound\" is not a very good choice by the LG team.\n",
    "bound = create_extractor(\n",
    "    model,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# PAY ATTENTION: bound is the name of the extractor we created before\n",
    "result = bound.invoke(\n",
    "    f\"\"\"Extract the preferences from the following conversation:\n",
    "<convo>\n",
    "{conversation}\n",
    "</convo>\"\"\"\n",
    ")\n",
    "\n",
    "# Extract the preferences\n",
    "result[\"responses\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e947d-a520-4520-a13b-90ab7b2498b5",
   "metadata": {},
   "source": [
    "## OK, this time our code worked. Let's see what just happened\n",
    "\n",
    "The previous code builds on the **TrustCall** example and demonstrates how to use it to **extract complex preferences**—specifically **Telegram and Trust Fall preferences**—from a conversation. Here’s a simple explanation step-by-step:\n",
    "\n",
    "#### Create an Extractor with TrustCall\n",
    "```python\n",
    "bound = create_extractor(\n",
    "    model,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "```\n",
    "- **Purpose**: Sets up a **TrustCall extractor** to work with the previously defined **`TelegramAndTrustFallPreferences` schema**.  \n",
    "- **Parameters**:  \n",
    "  - **`tools=[TelegramAndTrustFallPreferences]`**: Specifies the schema to use for extraction.  \n",
    "  - **`tool_choice=\"TelegramAndTrustFallPreferences\"`**: Ensures the AI focuses only on this schema, ignoring unrelated information.  \n",
    "\n",
    "This step prepares the extractor to **map data from the conversation** into the **complex schema** with nested preferences.\n",
    "\n",
    "#### Input Conversation\n",
    "```python\n",
    "conversation = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "```\n",
    "- This is a **simulated conversation** between an operator and a customer.  \n",
    "- It contains **hidden preferences** about:\n",
    "  1. **Telegram settings** (paper type, delivery speed).  \n",
    "  2. **Trust fall preferences** (fall height, catching technique).  \n",
    "  3. **Morse code preferences** (key type).  \n",
    "\n",
    "#### Invoke the Extractor\n",
    "```python\n",
    "result = bound.invoke(\n",
    "    f\"\"\"Extract the preferences from the following conversation:\n",
    "<convo>\n",
    "{conversation}\n",
    "</convo>\"\"\"\n",
    ")\n",
    "```\n",
    "- **Purpose**: Sends the conversation to the **TrustCall extractor** along with an instruction to **extract preferences**.  \n",
    "- **Message Format**: Wraps the conversation inside XML-like tags `<convo>` to **help the model focus** on the relevant content.  \n",
    "- **Result**: Outputs the extracted preferences in the specified schema format.\n",
    "\n",
    "#### Access the Extracted Preferences\n",
    "```python\n",
    "result[\"responses\"][0]\n",
    "```\n",
    "- Retrieves the **first response** from the result, which contains the **structured preferences** extracted from the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aaf6a4-ae81-4254-b26d-716b108a609b",
   "metadata": {},
   "source": [
    "## Let's talk about the response we got\n",
    "\n",
    "The output we received is a **structured representation** of the user's preferences extracted from the conversation, organized into the complex schema defined earlier. Let’s break it down step-by-step in simple terms:\n",
    "\n",
    "#### Top-Level Schema\n",
    "```python\n",
    "TelegramAndTrustFallPreferences(\n",
    "    pertinent_user_preferences=UserPreferences(\n",
    "```\n",
    "- **`TelegramAndTrustFallPreferences`**: The **main schema** wrapping all user preferences.  \n",
    "- **`pertinent_user_preferences`**: Contains user-specific preferences grouped into two categories:\n",
    "  1. **Communication Preferences** (Telegram, Morse Code, Semaphore).  \n",
    "  2. **Trust Fall Preferences** (height, techniques, etc.).\n",
    "\n",
    "#### Communication Preferences\n",
    "```python\n",
    "communication_preferences=CommunicationPreferences(\n",
    "```\n",
    "This section deals with **communication preferences**, broken down into three types:\n",
    "\n",
    "**a) Telegram Preferences**\n",
    "```python\n",
    "telegram=TelegramPreferences(\n",
    "    preferred_encoding=[\n",
    "        OutputFormat(preference='Morse', \n",
    "        sentence_preference_revealed='Morse code or standard encoding?')\n",
    "    ],\n",
    "    favorite_telegram_operators=None,\n",
    "    preferred_telegram_paper=[\n",
    "        OutputFormat(preference='Daredevil', \n",
    "        sentence_preference_revealed='Shall I use our \"Daredevil\" paper for this daring message?')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "- **Preferred Encoding**:  \n",
    "  - **Preference**: \"Morse\".  \n",
    "  - **Evidence**: Mentioned in: *\"Morse code or standard encoding?\"*  \n",
    "- **Favorite Telegram Operators**:  \n",
    "  - **`None`**—No preference provided.  \n",
    "- **Preferred Telegram Paper**:  \n",
    "  - **Preference**: \"Daredevil\".  \n",
    "  - **Evidence**: Mentioned in: *'Shall I use our \"Daredevil\" paper?'*  \n",
    "\n",
    "**b) Morse Code Preferences**\n",
    "```python\n",
    "morse_code=MorseCode(\n",
    "    preferred_key_type=[\n",
    "        OutputFormat(preference='straight key', \n",
    "        sentence_preference_revealed='I love using a straight key.')\n",
    "    ],\n",
    "    favorite_morse_abbreviations=None\n",
    ")\n",
    "```\n",
    "- **Preferred Key Type**:  \n",
    "  - **Preference**: \"straight key\".  \n",
    "  - **Evidence**: Mentioned in: *\"I love using a straight key.\"*  \n",
    "- **Favorite Morse Abbreviations**:  \n",
    "  - **`None`**—No preference provided.\n",
    "\n",
    "**c) Semaphore Preferences**\n",
    "```python\n",
    "semaphore=Semaphore(\n",
    "    preferred_flag_color=None, \n",
    "    semaphore_skill_level=None\n",
    ")\n",
    "```\n",
    "- **Preferred Flag Color**:  \n",
    "  - **`None`**—No mention of colors.  \n",
    "- **Semaphore Skill Level**:  \n",
    "  - **`None`**—No mention of skill levels.\n",
    "\n",
    "#### Trust Fall Preferences\n",
    "```python\n",
    "trust_fall_preferences=TrustFallPreferences(\n",
    "    preferred_fall_height=[\n",
    "        OutputFormat(preference='higher', \n",
    "        sentence_preference_revealed=\"I'm ready for a higher fall.\")\n",
    "    ],\n",
    "    trust_level=None,\n",
    "    preferred_catching_technique=[\n",
    "        OutputFormat(preference='diamond formation', \n",
    "        sentence_preference_revealed='I prefer the diamond formation for catching.')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "- **Preferred Fall Height**:  \n",
    "  - **Preference**: \"higher\".  \n",
    "  - **Evidence**: Mentioned in: *\"I'm ready for a higher fall.\"*  \n",
    "- **Trust Level**:  \n",
    "  - **`None`**—No explicit mention of trust levels.  \n",
    "- **Preferred Catching Technique**:  \n",
    "  - **Preference**: \"diamond formation\".  \n",
    "  - **Evidence**: Mentioned in: *\"I prefer the diamond formation for catching.\"*  \n",
    "\n",
    "#### Key Features Observed in Output\n",
    "\n",
    "1. **Highly Structured Data**:  \n",
    "   - Preferences are organized into distinct categories.  \n",
    "\n",
    "2. **Evidence Tracking**:  \n",
    "   - Each preference includes the **sentence it was extracted from**, making it **traceable**.  \n",
    "\n",
    "3. **Missing Values Handled Gracefully**:  \n",
    "   - Fields without data (e.g., semaphore preferences) are set to **`None`**, instead of causing errors.\n",
    "\n",
    "4. **Complex Schema Support**:  \n",
    "   - Nested schemas (like `CommunicationPreferences` and `TrustFallPreferences`) are supported seamlessly.\n",
    "\n",
    "#### 5. Final Thoughts\n",
    "- **Why is this Output Useful?**  \n",
    "  - It **preserves details** from the conversation in a **structured format** that can be saved in databases or used in future interactions.  \n",
    "  - It **avoids regenerating profiles** from scratch, focusing only on **updating specific fields** as needed.  \n",
    "\n",
    "This approach using **TrustCall** simplifies **complex schema extraction** while remaining **scalable and efficient** for AI-driven workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb52a3-d658-4127-99e8-280b9f19e0a1",
   "metadata": {},
   "source": [
    "## OK. Let's now update our previous chatbot with long-term memory adding TrustCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20e9e620-7af5-4ab4-882d-2c6509437733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    user_location: str = Field(description=\"The user's location\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\", # Enforces use of the UserProfile tool\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"      \n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "    \n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "    \n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    # Save the updated profile\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625be58-db00-47b7-ab5a-14dc8929d5dc",
   "metadata": {},
   "source": [
    "## How are we using TrustCall in the previous code?\n",
    "\n",
    "The previous code demonstrates how **TrustCall** is used to **extract, update, and store structured user profiles** in a chatbot with **memory capabilities**. It integrates **LangGraph** for workflow management and uses **TrustCall** for **schema-based data extraction**. Here’s a simple explanation step-by-step:\n",
    "\n",
    "#### Purpose of the Code\n",
    "This chatbot:\n",
    "1. **Extracts user information** (name, location, and interests) from conversations.  \n",
    "2. **Stores the extracted data** in memory for **personalization** in future chats.  \n",
    "3. **Updates memory incrementally** without regenerating the profile from scratch.  \n",
    "4. Uses **TrustCall** to handle structured extraction efficiently.\n",
    "\n",
    "#### Schema for User Profile\n",
    "```python\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    user_location: str = Field(description=\"The user's location\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")\n",
    "```\n",
    "- **Defines the data structure** for storing user details.  \n",
    "- Includes:\n",
    "  - **`user_name`**: User’s preferred name.  \n",
    "  - **`user_location`**: User’s location.  \n",
    "  - **`interests`**: List of user interests.  \n",
    "\n",
    "- Each field has a **description** to guide the AI when extracting information.\n",
    "\n",
    "#### Create the TrustCall Extractor\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\",\n",
    ")\n",
    "```\n",
    "- Sets up **TrustCall** to extract data based on the **`UserProfile` schema**.  \n",
    "- **`tool_choice`:** Ensures the AI only extracts fields specified in this schema, avoiding unrelated data.\n",
    "\n",
    "#### Workflow Overview\n",
    "\n",
    "**Step 1: Respond to Messages (call_model)**\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **Loads memory** for the user from the store.  \n",
    "- If memory exists, it formats the data to include:\n",
    "  - **Name**  \n",
    "  - **Location**  \n",
    "  - **Interests**  \n",
    "- The AI responds to the user while using this **memory for personalization**:\n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "```\n",
    "\n",
    "**Step 2: Update Memory (write_memory)**\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **Extracts new information** from the chat and **updates the memory**.  \n",
    "\n",
    "**Key Steps:**\n",
    "1. **Fetch Existing Memory:**\n",
    "```python\n",
    "existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "```\n",
    "- Retrieves the current profile if it exists.  \n",
    "\n",
    "2. **Invoke TrustCall Extractor:**\n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "```\n",
    "- Uses TrustCall to:\n",
    "  - Analyze chat messages.  \n",
    "  - Update only the **relevant fields** in the profile, leaving others unchanged.  \n",
    "\n",
    "3. **Save Updated Profile:**\n",
    "```python\n",
    "updated_profile = result[\"responses\"][0].model_dump()\n",
    "store.put(namespace, key, updated_profile)\n",
    "```\n",
    "- Converts the result into JSON and **saves it in memory**.\n",
    "\n",
    "#### Memory Management\n",
    "\n",
    "**Short-Term Memory (Session Memory)**\n",
    "```python\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- Keeps memory for **ongoing conversations** (temporary).  \n",
    "\n",
    "**Long-Term Memory (Persistent Memory)**\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "- Stores **user profiles persistently** for **future sessions**.  \n",
    "\n",
    "#### Workflow Graph\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- Defines a **state-based workflow** for the chatbot:\n",
    "1. **START → call_model**:  \n",
    "   - Respond to the user using existing memory.  \n",
    "2. **call_model → write_memory**:  \n",
    "   - Extract and update the user’s memory based on the latest conversation.  \n",
    "3. **write_memory → END**:  \n",
    "   - Store updated memory and end the workflow.\n",
    "\n",
    "**Graph Visualization**\n",
    "```python\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- Generates a **diagram** to visualize the chatbot’s workflow steps.\n",
    "\n",
    "#### Example Scenario\n",
    "\n",
    "**Input Conversation:**\n",
    "```\n",
    "Human: Hi, I'm Sarah.\n",
    "AI: Nice to meet you, Sarah.\n",
    "Human: I'm from New York and I love hiking and photography.\n",
    "```\n",
    "\n",
    "**Memory Update:**\n",
    "```\n",
    "{\n",
    "    \"user_name\": \"Sarah\",\n",
    "    \"user_location\": \"New York\",\n",
    "    \"interests\": [\"hiking\", \"photography\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Next Conversation:**\n",
    "```\n",
    "Human: Tell me about the best hiking trails near New York.\n",
    "AI: Since you love hiking and you're based in New York, let me suggest a few great trails nearby...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d96b4-09d1-43ca-beab-b71e3d47dbe9",
   "metadata": {},
   "source": [
    "## Well, let's see how this app works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "332d84b7-7c11-4c6e-aa38-2970dc1f6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "746309f6-f18b-4b40-87a2-d4f8f8080c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm interested in Gen AI Startups\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great, Julio! Generative AI is a rapidly growing field with a lot of exciting developments. Are you looking for information on specific startups, trends in the industry, or perhaps advice on getting involved with a generative AI startup? Let me know how I can help!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I'm interested in Gen AI Startups\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e7e46a9-d22b-470b-be39-2cb6ebdf09c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'user_name': 'Julio',\n",
       "  'user_location': '',\n",
       "  'interests': ['Gen AI Startups']},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2025-01-07T10:04:01.754010+00:00',\n",
       " 'updated_at': '2025-01-07T10:04:01.754012+00:00'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54808e5b-e570-4bf6-a93f-93bee95c3316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Julio', 'user_location': '', 'interests': ['Gen AI Startups']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The user profile saved as a JSON object\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf6357ed-1dcc-4150-85d4-a1e6f7f2ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I am also interested in the latest trends about AI Agents\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "AI agents are evolving rapidly, and several key trends are shaping their development and application:\n",
      "\n",
      "1. **Increased Autonomy**: AI agents are becoming more autonomous, capable of performing complex tasks without human intervention. This includes decision-making, problem-solving, and adapting to new environments.\n",
      "\n",
      "2. **Multi-Modal Capabilities**: AI agents are increasingly integrating multiple types of data, such as text, images, and audio, to provide more comprehensive and context-aware interactions.\n",
      "\n",
      "3. **Personalization**: There's a growing focus on creating AI agents that can tailor their interactions and responses based on individual user preferences and behaviors, enhancing user experience.\n",
      "\n",
      "4. **Ethical and Responsible AI**: As AI agents become more prevalent, there's a stronger emphasis on ensuring they operate ethically, with transparency and accountability in their decision-making processes.\n",
      "\n",
      "5. **Collaboration with Humans**: AI agents are being designed to work alongside humans, augmenting human capabilities rather than replacing them. This includes applications in customer service, healthcare, and education.\n",
      "\n",
      "6. **Real-Time Learning and Adaptation**: AI agents are being developed to learn and adapt in real-time, allowing them to improve their performance and relevance over time.\n",
      "\n",
      "7. **Integration with IoT and Smart Devices**: AI agents are increasingly being integrated with Internet of Things (IoT) devices, enabling smarter home and industrial automation solutions.\n",
      "\n",
      "These trends indicate a future where AI agents will play a more significant role in both personal and professional settings, offering enhanced capabilities and efficiencies. If there's a particular trend or application you're curious about, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I am also interested in the latest trends about AI Agents\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6245c-e319-4633-8dfb-69eb118ca2d1",
   "metadata": {},
   "source": [
    "## Let's review what we just did\n",
    "\n",
    "This code demonstrates how the chatbot **collects, updates, and retrieves user preferences** using the **graph workflow** and **TrustCall extractor** set up earlier. It uses both **short-term (session-specific)** and **long-term (persistent)** memory to **personalize responses** and **incrementally update the user profile**. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Configure Memory Settings\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **Thread ID (`thread_id`)**: Tracks **short-term memory** within the current conversation (session).  \n",
    "- **User ID (`user_id`)**: Tracks **long-term memory** across multiple sessions.  \n",
    "- Ensures data is **stored and retrieved** based on the user ID (\"1\").\n",
    "\n",
    "#### First User Input - Name\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**What happens here?**\n",
    "1. **Input Message**: The user says their name is \"Julio\".  \n",
    "2. **Graph Workflow**:\n",
    "   - **Step 1:** Responds to the user using existing memory (currently empty).  \n",
    "   - **Step 2:** Updates the memory to save **\"Julio\"** as the **user_name**.  \n",
    "3. **Output**: The AI responds, e.g., *\"Hi Julio! Nice to meet you!\"*  \n",
    "\n",
    "\n",
    "#### Second User Input - Interest in Gen AI Startups\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"I'm interested in Gen AI Startups\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**What happens here?**\n",
    "1. **Input Message**: The user adds their interest in **Gen AI Startups**.  \n",
    "2. **Graph Workflow**:\n",
    "   - **Step 1:** Responds based on existing memory (user_name = Julio).  \n",
    "   - **Step 2:** Updates the memory, adding **\"Gen AI Startups\"** to the **interests** list.  \n",
    "3. **Output**: The AI responds, e.g., *\"That's great, Julio! Gen AI Startups are fascinating.\"*\n",
    "\n",
    "\n",
    "#### Retrieve and Inspect Memory\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()\n",
    "existing_memory.value\n",
    "```\n",
    "\n",
    "**What happens here?**\n",
    "1. **Retrieve Memory**:\n",
    "   - Fetches the saved profile for the user (ID = 1) from **long-term memory**.  \n",
    "2. **Memory Contents**:\n",
    "   - Displays the profile as a **JSON object**:\n",
    "   ```json\n",
    "   {\n",
    "       \"user_name\": \"Julio\",\n",
    "       \"user_location\": null,\n",
    "       \"interests\": [\"Gen AI Startups\"]\n",
    "   }\n",
    "   ```\n",
    "3. **Key Observations**:\n",
    "   - **Incremental Updates**: Memory only added **new data** (interest in Gen AI Startups) without overwriting existing data (user_name = Julio).  \n",
    "   - **Preserves Missing Fields**: Keeps fields like **location** as **null** if no information is provided.\n",
    "\n",
    "\n",
    "#### Third User Input - Additional Interest\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"I am also interested in the latest trends about AI Agents\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    "**What happens here?**\n",
    "1. **Input Message**: The user adds **AI Agents** as another interest.  \n",
    "2. **Graph Workflow**:\n",
    "   - **Step 1:** Responds using the updated memory (user_name = Julio, interests = [\"Gen AI Startups\"]).  \n",
    "   - **Step 2:** Updates the memory, **appending** **\"AI Agents\"** to the **interests** list.  \n",
    "3. **Output**: The AI responds, e.g., *\"AI Agents are a hot topic, Julio! Great addition to your interests.\"*\n",
    "\n",
    "\n",
    "#### Updated Memory After Third Input\n",
    "```json\n",
    "{\n",
    "    \"user_name\": \"Julio\",\n",
    "    \"user_location\": null,\n",
    "    \"interests\": [\"Gen AI Startups\", \"AI Agents\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "This code demonstrates how the chatbot:\n",
    "1. **Remembers user preferences** across sessions using memory.  \n",
    "2. **Adds new information incrementally** without overwriting old data.  \n",
    "3. **Extracts and organizes user details** using **TrustCall** into a **structured schema**.  \n",
    "4. **Personalizes interactions**, making conversations more natural and user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d853e2-a4a2-4d47-aa80-04e1eab9f8b8",
   "metadata": {},
   "source": [
    "## Looks good! Once we know that our chatbot has this more structured long-time memory, we can now continue our conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a57cab59-1f79-42e0-9e99-92525904d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What online magazines do you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Given your interests in Gen AI Startups and AI Agent Startups, I recommend the following online magazines and publications:\n",
      "\n",
      "1. **TechCrunch** - They frequently cover the latest in technology startups, including AI and machine learning innovations.\n",
      "\n",
      "2. **VentureBeat** - Known for its focus on transformative technology, VentureBeat often features articles on AI advancements and startup news.\n",
      "\n",
      "3. **Wired** - While it covers a broad range of tech topics, Wired often delves into AI and the impact of technology on society.\n",
      "\n",
      "4. **MIT Technology Review** - Offers in-depth articles on emerging technologies, including AI and its applications in various industries.\n",
      "\n",
      "5. **AI Trends** - Specifically focused on AI, this publication provides insights into the latest trends and developments in the AI space.\n",
      "\n",
      "6. **The Verge** - Covers the intersection of technology, science, art, and culture, with frequent articles on AI and tech startups.\n",
      "\n",
      "These publications should provide you with a wealth of information and insights into the areas you're interested in.\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What online magazines do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 026-profile-schema.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 026-profile-schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
