{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "# Map-Reduce operations with Send\n",
    "* Map-reduce operations are essential for efficient task decomposition and parallel processing.\n",
    "    * Map: Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "    * Reduce: Aggregate the results across all of the completed, parallelized sub-tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89c20f-6d77-4787-8a68-9d9fad6c02eb",
   "metadata": {},
   "source": [
    "## Ummm... What is a Map-Reduce operation again??? \n",
    "\n",
    "If you are unfamiliar with this name and it sounds confusing to you, do not worry. The real meaning is easier to understand than the weird name. \n",
    "\n",
    "#### Map \n",
    "Think of it as **splitting** a big job into smaller, manageable tasks. Each smaller task can be worked on at the same time (in **parallel**).  \n",
    "\n",
    "Example:  \n",
    "* If you have to count the words in 100 books, you can give 10 books to 10 different people, and each person counts the words in their assigned books.\n",
    "\n",
    "#### Reduce  \n",
    "Once all the smaller tasks are finished, the results are **combined** to produce the final output.  \n",
    "\n",
    "Example:  \n",
    "* The 10 people report their word counts, and you **add up** all the counts to get the total number of words in the 100 books.\n",
    "\n",
    "\n",
    "#### Map-Reduce Processing \n",
    "- **Map**: Divides a big task into smaller tasks.  \n",
    "- **Reduce**: Combines the results of smaller tasks into the final output.\n",
    "\n",
    "This approach saves time by **distributing work** efficiently, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c390c-2ea8-4ecf-a2a1-640ef2382220",
   "metadata": {},
   "source": [
    "## And how a map-reduce operation is done in LangGraph?\n",
    "In LangGraph, the map-reduce pattern is a method for handling tasks by breaking them into smaller sub-tasks, processing these sub-tasks simultaneously, and then combining the results. This approach is particularly useful when the number of sub-tasks isn't known in advance.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. **Mapping:** Start with a main task that generates a list of items. For example, if given a general topic, the system might produce a list of related subjects.\n",
    "\n",
    "2. **Processing in Parallel:** Each item from the list is then processed independently and at the same time. Continuing the example, for each related subject, the system could generate a joke.\n",
    "\n",
    "3. **Reducing:** After all items have been processed, the results are combined or summarized. In our case, this could involve selecting the best joke from the list of generated jokes.\n",
    "\n",
    "#### Challenges Addressed\n",
    "\n",
    "- **Unknown Number of Sub-Tasks:** Since the number of items (like related subjects) may not be known beforehand, it's challenging to set up the workflow in advance.\n",
    "\n",
    "- **Dynamic Input States:** Each sub-task might require a different input, necessitating a system that can handle varying inputs dynamically.\n",
    "\n",
    "#### The LangGraph's Solution: using Send\n",
    "\n",
    "As we will see below, **LangGraph uses the built-in Send API to manage these challenges**. By employing conditional edges, the Send function can distribute different inputs to multiple instances of a processing node. This means each sub-task can receive a unique input, allowing for flexible and dynamic workflow management.\n",
    "\n",
    "This map-reduce approach in LangGraph enables efficient task decomposition and parallel processing, making it easier to handle complex workflows where the number of sub-tasks isn't predetermined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7fdeb-2543-4280-be25-8e962552530c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 022-map-reduce.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2690994-1b84-43f9-a8bf-d10036971be4",
   "metadata": {},
   "source": [
    "## Basic Map-Reduce operation example: A Joke Generator App\n",
    "\n",
    "#### The map-reduce process\n",
    "We will design an app that will do two things:\n",
    "* Create a set of jokes from subjects or sub-topics related to a topic (map operation).\n",
    "* Pick the best joke from the list (reduce operation).\n",
    "\n",
    "#### The app workflow will be:\n",
    "* The user will input a topic.\n",
    "* The app will generate a list of subjects or sub-topics related to the topic.\n",
    "* The app will generate a joke for each subject.\n",
    "* The app will select the best joke generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb3d15-ac48-4c7e-bc72-2a75b2f74754",
   "metadata": {},
   "source": [
    "## Let's start by creating the prompts we will use with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f3e2e3-5f7f-458d-ac64-eba955b01ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompts we will use\n",
    "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d5fc1-55e4-4b2e-8bbe-1c46d689529e",
   "metadata": {},
   "source": [
    "## Let's now define the schemas that will define the format of the Subjects, BestJoke, and OverallState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b0f650-8c60-4c4c-832c-34ca67accbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "\n",
    "# PAY ATTENTION HERE: The `jokes` key will append the jokes.\n",
    "# See how we are using the operator.add reducer.\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a727b-f7a4-4c78-a6e4-535c696e3c78",
   "metadata": {},
   "source": [
    "## Now we will define the function to generate subjects or sub-topics related to the topic submitted by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3494781b-094f-476b-93cd-f7d9b4511b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are asking the LLM to generate subjects related with the topic\n",
    "def generate_subjects(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65e97f-7c39-4bd4-921a-957ed3cc7081",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "Here’s a simple explanation of the previous code:\n",
    "\n",
    "1. **Purpose**: The function `generate_subjects` is designed to use a LLM (referred to as `model`) to generate a list of subjects related to a given topic.\n",
    "\n",
    "2. **Input**: It takes in a single argument called `state`, which is a dictionary-like object (`OverallState`). This `state` contains a key `\"topic\"` that specifies the main topic for which related subjects should be generated.\n",
    "\n",
    "3. **Steps**:\n",
    "   - **Create a Prompt**: The `subjects_prompt.format(topic=state[\"topic\"])` takes a predefined prompt template (`subjects_prompt`) and fills in the topic from the `state`.\n",
    "   - **Ask the Model**: The LLM (`model`) is used to process this prompt. The method `with_structured_output(Subjects).invoke(prompt)` ensures the model's response is structured and follows a specific format defined in `Subjects`.\n",
    "   - **Extract Subjects**: From the model's response, the `subjects` field is extracted.\n",
    "\n",
    "4. **Output**: The function returns a dictionary with a single key, `\"subjects\"`, containing the list of subjects generated by the model.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "If the topic in `state[\"topic\"]` is `\"climate change\"`, the function:\n",
    "1. Formats a prompt like: \"Generate a list of subjects related to climate change.\"\n",
    "2. Sends this prompt to the model.\n",
    "3. Receives and processes the model's response (e.g., `[\"Global Warming\", \"Renewable Energy\", \"Carbon Footprint\"]`).\n",
    "4. Returns: `{\"subjects\": [\"Global Warming\", \"Renewable Energy\", \"Carbon Footprint\"]}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bae6c3-5305-448d-8963-7fb20c6b737f",
   "metadata": {},
   "source": [
    "## Once we have the list of subjects, we will use Send to create a joke for each subject\n",
    "\n",
    "In simple terms, the **`Send`** function in LangGraph is used to dynamically send data (called **state**) to specific nodes in a workflow, even when you don't know in advance how many nodes you'll need or what data each node will receive.  \n",
    "\n",
    "#### Example Scenario:  \n",
    "Imagine you have a process where:  \n",
    "1. **Node A** creates a list of subjects (e.g., [\"cats\", \"dogs\", \"robots\"]).  \n",
    "2. You want to send each subject to another node (**\"generate_joke\"**) that generates a joke about that subject.  \n",
    "\n",
    "However, you don't know how many subjects there will be ahead of time.  \n",
    "\n",
    "#### What Does `Send` Do?  \n",
    "- It lets you create as many connections as needed **on the fly** (instead of predefining them).  \n",
    "- Each connection can have its **own data** (each particular subject).  \n",
    "\n",
    "#### Step 1: using a conditional edge, connect the \"Node A\" with \"the Send Magic\".\n",
    "```python\n",
    "graph.add_conditional_edges(\"node_a\", continue_to_jokes)\n",
    "```\n",
    "- **Connects the \"node a\" to the \"generate_joke\" node dynamically, creating a joke from the associated subject**. \n",
    "\n",
    "#### Step 2: \"the Send Magic\" creates as many generate_joke parallel nodes as many subjects were generated by the Node A\n",
    "```python\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n",
    "```\n",
    "- This function takes the state (e.g., a list of subjects).  \n",
    "- It creates a Send object **for each subject, telling it**:  \n",
    "  1. **Go to the `generate_joke` node**.  \n",
    "  2. **Pass a specific subject (e.g., {\"subject\": \"cats\"}) as input**.   \n",
    "\n",
    "#### Key Idea:\n",
    "**Instead of defining fixed connections, `Send` allows the workflow to adapt based on the data at runtime**. This is especially useful for tasks like processing lists or batches of data.\n",
    "\n",
    "#### How we will use Send in our exercise\n",
    "* We will use [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) to create a joke for each subject. **This can automatically parallelize joke generation for any number of subjects**.\n",
    "    * `generate_joke`: the name of the node in the graph\n",
    "    * `{\"subject\": s`}: the state to send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1bb3df-06a7-4145-a8b9-a7f64cf4bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264e65f-2c54-46cc-a597-d11c970de8ac",
   "metadata": {},
   "source": [
    "## Explanation of the previous code in simple terms\n",
    "The code defines a function called **`continue_to_jokes`** that takes some input data (**state**) and prepares instructions to send each item in the data to a specific processing step.\n",
    "\n",
    "#### Breaking it down:\n",
    "1. **Input: `state`**  \n",
    "   - The input, called **`state`**, contains data organized like a dictionary.  \n",
    "   - It has a key called **\"subjects\"**, which is expected to be a **list** (e.g., `[\"cats\", \"dogs\", \"robots\"]`).\n",
    "\n",
    "2. **Output: List of `Send` objects**  \n",
    "   - For each subject in the list, the function creates a **`Send`** object.  \n",
    "   - Each **`Send`** object specifies:  \n",
    "     1. Go to the **\"generate_joke\"** node (a step in the workflow).  \n",
    "     2. Pass a specific **subject** as input, like `{\"subject\": \"cats\"}`.\n",
    "\n",
    "3. **Purpose**  \n",
    "   - The function is used to **dynamically route data**.  \n",
    "   - If there are 3 subjects in the list, it creates 3 `Send` instructions—one for each subject—telling the system to process each item individually.\n",
    "\n",
    "#### Example:  \n",
    "If **state[\"subjects\"] = [\"cats\", \"dogs\"]**, the output will be:  \n",
    "```\n",
    "[\n",
    "    Send(\"generate_joke\", {\"subject\": \"cats\"}),  \n",
    "    Send(\"generate_joke\", {\"subject\": \"dogs\"})\n",
    "]\n",
    "```\n",
    "\n",
    "This allows the workflow to handle multiple inputs (subjects) in parallel, sending each one to the **\"generate_joke\"** node for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f0633-0d2f-4e04-9216-9cdea584fbd7",
   "metadata": {},
   "source": [
    "## Ooops! But we still have not defined the joke creator function. Let's do it now.\n",
    "* We will define a node that will create our jokes, `generate_joke`.\n",
    "* We will return them as `jokes` in `OverallState`.\n",
    "* This key has a reducer that will combine lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0586732f-51bd-4971-9319-c40216598ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "# PAY ATTENTION HERE: we ask the LLM to generate a joke about a subject\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f9352-4cad-437e-8953-8f08b29a1412",
   "metadata": {},
   "source": [
    "## Let's explain the previos code in simple terms\n",
    "The previous code defines how to generate a joke about a specific subject using an LLM. Let me break it down step by step:\n",
    "\n",
    "#### Defining Data Structures\n",
    "- `JokeState` and `Joke` are like blueprints for organizing data.\n",
    "\n",
    "  - `JokeState` tells us that we need a dictionary (or an object) with a key called `subject` that holds a string. For example:\n",
    "    ```python\n",
    "    state = {\"subject\": \"cats\"}\n",
    "    ```\n",
    "  \n",
    "  - `Joke` defines the expected output format: it contains a single key, `joke`, which is also a string. For example:\n",
    "    ```python\n",
    "    {\"joke\": \"Why did the cat sit on the computer? To keep an eye on the mouse!\"}\n",
    "    ```\n",
    "\n",
    "#### The `generate_joke` Function\n",
    "This is the main function that creates a joke based on the given subject. Here’s what happens inside:\n",
    "\n",
    "1. **Getting the subject**:\n",
    "   - The `state` parameter is expected to contain the subject for the joke (like `\"cats\"`).\n",
    "   \n",
    "2. **Preparing a prompt**:\n",
    "   - The variable `prompt` is created using a pre-defined template (`joke_prompt`), where the subject is inserted. For example, if `state[\"subject\"]` is `\"cats\"`, the prompt would look like:\n",
    "     ```python\n",
    "     \"Tell me a funny joke about cats.\"\n",
    "     ```\n",
    "\n",
    "3. **Calling the Language Model (LLM)**:\n",
    "   - The `model.with_structured_output(Joke).invoke(prompt)` sends the prompt to the LLM.\n",
    "   - The LLM generates a response that matches the `Joke` structure, ensuring it returns a dictionary with a `joke` key.\n",
    "\n",
    "4. **Returning the Joke**:\n",
    "   - The response from the LLM is wrapped in another dictionary under the key `\"jokes\"`, so the final output might look like:\n",
    "     ```python\n",
    "     {\"jokes\": [\"Why did the cat sit on the computer? To keep an eye on the mouse!\"]}\n",
    "     ```\n",
    "\n",
    "#### In Summary\n",
    "- The function takes a subject (e.g., `\"cats\"`).\n",
    "- It creates a prompt asking the LLM to generate a joke about that subject.\n",
    "- The LLM responds with a joke.\n",
    "- The function returns the joke in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26be68-b6dc-482f-8c2f-6674d7dd819d",
   "metadata": {},
   "source": [
    "## And, finally, let's define the fuction to select the best joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656d3f71-c579-42f4-9cc5-47ec22739f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAY ATTENTION HERE: we ask the LLM to select the best joke\n",
    "def best_joke(state: OverallState):\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add83d0d-51e5-47b7-8fbd-b3af4a0067dd",
   "metadata": {},
   "source": [
    "## OK. Let's review what we just did\n",
    "The previous code defines a function that helps pick the best joke from a list of jokes using a language model (LLM). Let’s break it down step by step:\n",
    "\n",
    "\n",
    "#### Input Data (`state`)\n",
    "- The function works with a dictionary called `state` (which follows the `OverallState` structure). It contains two key pieces of information:\n",
    "  1. **`state[\"jokes\"]`**: A list of jokes (e.g., `[\"Why did the chicken cross the road? To get to the other side!\", \"What do you call fake spaghetti? An impasta!\"]`).\n",
    "  2. **`state[\"topic\"]`**: The topic the jokes are related to (e.g., `\"animals\"`).\n",
    "\n",
    "\n",
    "#### Joining the Jokes\n",
    "- The jokes from `state[\"jokes\"]` are combined into a single string, separated by double newlines (`\\n\\n`). For example:\n",
    "  ```python\n",
    "  jokes = \"Why did the chicken cross the road? To get to the other side!\\n\\nWhat do you call fake spaghetti? An impasta!\"\n",
    "  ```\n",
    "\n",
    "\n",
    "#### Creating the Prompt\n",
    "- The function builds a prompt using a template (`best_joke_prompt`) to ask the LLM which joke is the best. It includes:\n",
    "  - The **topic** of the jokes.\n",
    "  - The **list of jokes**.\n",
    "\n",
    "The prompt would be something like:\n",
    "  ```text\n",
    "  \"Here are some jokes about animals:\n",
    "  \n",
    "  1. Why did the chicken cross the road? To get to the other side!\n",
    "  \n",
    "  2. What do you call fake spaghetti? An impasta!\n",
    "  \n",
    "  Which joke is the best? Respond with the joke number.\"\n",
    "  ```\n",
    "\n",
    "\n",
    "#### Asking the LLM\n",
    "- The `model.with_structured_output(BestJoke).invoke(prompt)` sends the prompt to the LLM and asks it to select the best joke. \n",
    "- The `BestJoke` structure ensures the response includes an `id`, which identifies the number or position of the selected joke.\n",
    "\n",
    "\n",
    "#### Returning the Best Joke\n",
    "- The function takes the `id` from the LLM’s response and uses it to look up the corresponding joke in the `state[\"jokes\"]` list.\n",
    "- The best joke is returned in a dictionary under the key `\"best_selected_joke\"`. For example:\n",
    "  ```python\n",
    "  {\"best_selected_joke\": \"Why did the chicken cross the road? To get to the other side!\"}\n",
    "  ```\n",
    "\n",
    "\n",
    "#### In Summary\n",
    "- The function takes a list of jokes and a topic.\n",
    "- It asks the LLM to pick the best joke by providing a clear prompt.\n",
    "- It returns the selected joke in an easy-to-use format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14772711-8682-47f7-b622-4b95871cb097",
   "metadata": {},
   "source": [
    "## OK. Let's now put it all together in our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c58107-485e-46f7-8484-8b984f2b38f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAAGwCAIAAACCV6iAAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdAE+f/B/Ang+xJAmGEKSg4QcCJeytYxVEHbq1tHfXXarUtjmq1ttqWKs66WkVrbbV1i0rVioCKC0RBEFA2JEDInr8/7r5pqgfFErgDntdfcLlcPkneeZ7nNslisQAI+icy3gVARARjAWGAsYAwwFhAGGAsIAwwFhAGKt4FvIHSfK261qiuNZmMFp3GjHc5DUJnkml0MotHYfGoTu50vMtpqBYQi6y7tc/TlXmPVV4d2cACWFyKUEIDLWRri9FgqSjSqBUmBptcmK3x6cz26cLyDuTgXde/IBF5c1ZGUs2tc5XeHdm+XTg+ndgUKgnvihpFpTDmZajKCrTlL3V9IkVegWy8K6oTQWNRUaS7eKhU2p7ZN1JMY7S2AVBlse7WGRmdSR4x0wXvWrARMRZP7ygeXKseM9+VK3TAu5YmVJKv+e37oqkrPERuhBtzEC4Wz9OVuY9Uw6ZL8C6kmRz9+kXkAsL9AIgVi7QrVZUluhEzCNq0NpFjW14MmODk5svEu5C/Eajbzs9UFT/XtLVMAACmrvA8s7dYryXQKjdRYlFbZXh8qybyHTe8C8HH9FWeCUdK8a7ib0SJxc3fKzuE8fCuAjccgQNP5PDwejXehaAIEYvyF1pFldGvG9E38jSpvpHipDOVeFeBIkQsMpJr+o0T410FzihUUvhb4gfXCNFg4B8Lvdb87L6y2cbhSqXy6dOneD29fu7tmJm3FU208DeCfyyeZyh9Ozdf9zFlypQ//vgDr6fXT+RGN+jMCpmhiZbfcPjHouS51i+4+WKh1+v/2xORDTz/+ekNFNiDW/BU3aQv0RD4x6I0X8sVNsmO3EOHDo0ePTo8PHzevHm3b98GAERERMjl8hMnToSGhkZERCBf844dO8aOHduzZ88xY8bs3LnTZDIhT//qq6+GDx9+48aN8ePHh4aG3rlz5/Wn2x2TQ5UV65piyW8E/x3r6loTi0ux+2Jv374dFxc3cuTIPn363Lp1S61WAwC+/vrrxYsXh4SETJ8+nUajAQAoFEpqamr//v2lUmlWVtaBAwd4PF50dDSyEKVSuXPnzlWrVmk0mrCwsNefbndsHuXFU1NTLPmN4BwLi8WiUZpYXPuXUVxcDACYPHly165dR48ejUzs2LEjlUoVi8VBQUHIFAqF8uOPP5JI6C77wsLCxMREayz0en1MTEznzp3rerrdsXlUlcLYRAtvOJxjYTJa2Dz7NxUAgPDwcB6Pt3r16hUrVoSHh9czp1wu/+GHH1JSUhQKBQCAy+VaH2IwGNZMNA8KFTg44N+z41wB1YFsNFi0avs3m2Kx+MCBA15eXsuWLZs3b155eTnmbDKZbPr06bdv337vvfe2b98eGBhoHVsAAFgslt0Lq5+yxkSl4X+0Ef7BZHEp6tom6U29vb23bdu2a9eunJycdevWWafb7jT+7bff5HL5zp07R4wY0alTJxeXf99R16T7nFUKI5uH/4AP/1i4tWNqapukN0VWJsPCwvr162fdBsVkMisr/97GXF1dLRQKrWmorq6u/1t/5el2Z9CaRa5NMph9IxTbnxEuaqsMxTla7452PrDx8ePHCxYsMBqNz549O3nyZMeOHZGBZ1ZWVmJiIpVKff78uYODA5vNPn36tMlkMhgMP/7449WrV1Uq1aRJkxgMRlJSUl5e3owZM2wX+8rTHR0d7Vv2nycqggYIWHg3GPjHgs2jJp2uDB4otO9ia2pqsrOzExISbt++3b17908//ZTD4QAAunbtmpWVdf78+adPn3bq1Gnw4MFms/nEiRNXr1718PBYvXr1/fv31Wp1aGgoZixeebqPj48da1bIDY9vKfpE4r97iBBHZ138sSR0mKOYeIc0NrMntxW1ckOPkSK8C8F7BRXRIZSXck4WsaDOY3C++OKLK1euvD5dIpGUlZW9Pp3P5zfdngurmzdvxsTEYD4klUoLCwtfn37kyBGpVFrXAv86VTlrjZdda/yPCNFaAAB+21bYO0JU137U6upqZDPlKwwGg4MDxsGxZDK5IesUjaTVauVyOeZDJBL2B+vs7EylYv8U065U6bSmPhH49yAEikVJniYzRTFkals54Pt1J+MKxy9yt25vxRf+K6gIVx+m2J1+42QF3oXg4/jWl+HjxATJBIFiAQDo1l9g0JvvXMZulluxc/tLuvbnO0sZeBfyN6J0IlZ3LslJZBA6zM7bAwjr/IGSrv34Uv/m3spePwK1FoiwEY46jZlQR8c3Eb3WfPSrF35BHKJlgoitBSLrbu2NU+U9R4m6hgvwrsX+LGZL0hlZWYF24CQnkSsRt9YQNBYAAL3WlHxWnv9E1bkP37czWyjBf09B45XkaYpyNCkX5H0jRcGD7Lxh146IGwuEstr46K/q5xkqixn4dGZTHUhsPpUrpJoJdGZefUiApJDpVQojIIHMZIXAmeYXxA4aQNxAIIgeC6vqCn1JnlZZbVTVGMlUcq3czodHFxQUsNlssdjOW5PYfCqZAtg8Kk9ElfqzmOwmOebI7gix8bshBE40gVMT9iNffLHfrUPnMeO6NN1LtCCEWxOBiADGAsIAY4ESCoV0OhHXFXEBY4GqqqrS6fA/b4cgYCxQdDqdQmkZqwnNAMYCpdPpbE8FaONgLFBsNruuA2TaIBgLlEqlMhrxP8uPIGAsUI6OjnBNxArGAiWXy+GaiBWMBYQBxgLFYDDgCqoVjAVKq9XCFVQrGAsUbC1swVigYGthC8YCwgBjgeLz+U10lbSWCMYCVVNT09TX3GxBYCwgDDAWKLjx2xaMBQpu/LYFYwFhgLFAiUQi2IlYwVigZDIZ7ESsYCwgDDAWKHhCgC0YCxQ8IcAWjAWEAcYCBc8TsQVjgYLnidiCsUDBPai2YCxQcA+qLRgLCAOMBYrFYsGTDa1gLFBqtRqebGgFY4GCx1vYgrFAweMtbMFYoGBrYQvGAgVbC1swFigOh4N5I6O2qcVcxbeJDB06lMlkAgBqa2spFApym2QqlXrq1Cm8S8NTW19TF4lEubm51n9ramosFktkZCSuReGvrXciM2bMeGVXiIuLS3R0NH4VEUJbj0VERISHh4ftlO7du/v5+eFXESG09VgAAKZPn25tMCQSydy5c/GuCH8wFmDs2LG+vr4AAIvFEhoaat/7Y7dQMBYAaTBYLJZEIpk1axbetRDCv6+JGHRmWYlerWzNRy518Ojf0TvFzc0NqCXPM1R4l9NUSCTAE1IFzjQK9V9uuPov2y1unKzIeaBk86lMTltflW0FGGxKRaHWgU7q2IPXuS+/njnri8WFgyVCV0an3kS/sRb0RiwWS9KpMldfRtCAOu8aWWcsLseXCST0gLBWeL9JCADw18lSrwBmp97YbQb2kLPspVarMcNMtGJ9xjo/Tqk1m7AbBexYyEv0VAe4ktKaUahkrcqkqOMGkdjfvUphFIjh0fGtnJMHo0b2JrEwm4DJ2Kb3rLYFWpUJWLDXVGFPAWGAsYAwwFhAGGAsIAwwFhAGGAsIA4wFhAHGAsIAYwFhgLGAMMBYQBhacyxMJlN6+oPmfMVffzs6aEioWq2uq56Zsyfs2PltY14i80lGM5wr25pjseWbDd/GbsK7ir+RSCQOh8tgMP7zEi5eOrNo8WytVmPXujA01RGahYUvpFLPJlq4lcViIZHqPFpVT7Az0Mlk8s64Q41ZQrOdU2+3WMhkldvjtqSlpVIdHEJCet64cXXPriM+Pu0AAH+c/vWXE0cqK8tdXNyGDB759uQZdDr9WU7WkqVzN2/atnff9tzcbInEdeGCpX37DkCWVlJavHPnt2n3Umk0env/gLlz3w/o0BEA8P22r67fuLr8w5idu78rKnq5dctOD6nX/oM7U1OTVCqlh4fXtKlzhg4ZCQDY/PW6P69dBgAMGhIKADgaf9rVxQ0AcP/B3R/2xeXmZguFjsFBYfPnLRKJxPW8r5SUm3v3bS8uLnRxcRsbOTFq/Nt301JXfLxox/aDHTt2QeYZNSZ8/Li331mwBPl33/64G38lajTq0JBe77/3oUTiAgC4fPn8ps1rAADt/QP27D6CzFlPMecv/HHy1M8vXuRzONw+vfvPm/t+6u2k2O83AwDGRQ0FAKz8eO3IEZGvl2eXb9M+sTCZTJ9+tkxeJfvgg1VyeeUP++KCg0KRTBz6ce+JX49EjZ/i5eX78mX+8V9+Kix68emq9Uj2P9+wasniFa4ubgcP7f5i02c/Hz3L5wtkssolS+e6u3ssXrScRCIlJJz7YNn83TsPIwtUqZT7D+5c9sEqrVbTPTispLT46dPHb42dyOcJbtxM3Lgpxt3dIzCgU/S0uRXlZSUlRZ+sWg8AEDmKAQBp926v+mTpsKGjx497u1ZR89vJYx8uf3fPriN1NexqtXrd+pXeXr4ffRiTl5cjk1U05NOoqChfMG/x87ycU78fz8rO/GHvMS6H261byIb1W+OPHjT/76Kw9RRz6Mc9P/70w8ABQydNmF5VLb9zJ5nq4NCzR9/Jk6J/OXHky42xbDZHKvX8b+U1hH1i8eRJRvazp2vXbB44YCgA4MWL/AsXT+v1eoWiJv7ogZjPNg7oPwSZUyRy+i72y8WLliP/Llm8YvCg4QCA+fMXL3w3+uGje/37DT58ZJ9Q4PjNll3Ipe+GDR0dPXPc2fOnlixaDgDQ6/XLP4wJDOyMLMHN1f3QgRNIVzJq1FvjJwxNSroWGNBJKvXk8wXyKlmXLkHWOrfHbYmMiFq65GPk39DQXrPmTLxzN7lf+CDM91VVLdfpdP36DR42dFTDP41PVq1HLogQ1C3k05j/O3ny51kzFzg7S5ydJRcvnakoL6u/mIAOnY7EHxg2bDTy4wEATHl7Jvpm3aQAgMDAzny+AABQVFz4H8prCPvEoryizFo0AEAq9TSbzRqNOi0t1Wg0btwUs3FTDPIQcqB5ZUU58i+TwUT+kEhcAQCVlRUAgNTUpPKKstER/azLNxgM1k+TwWBYM4HIyc0+9OOerKxMpN2Sy2WYRZaWlhQU5BUVvTx77h/Xrij/35Jf5+bq3qlT1yPx+xkMZmRE1Jte5rd3734uEtcHD+7Omrmg4cWoVEqTyfRW5MR/XX4jy6uHfWLh7u4BAEhPf9DePwBpPMRiJz5fIJNXAgA2bYx1dpLYzu/mJs3Lz7Wd4kB1AACYzSYAgLxK1rt3v3fmL7Gdgc3mIH8wmSzb6ffu31m5aklwUOjHK9ayWew161aYLWbMIquqZACAWTPf6d9vsO10R8c6xxYkEmnzpm379sft3hN74tcjn6xc361b9zf5YIDYyVmlUr5RMafP/AoAcPrnJ9ZE5dXFPrHo0D4wLLTX3h+2lZWVVNdUJd26HvPZRgAAl8tDZvD09G740rhcXk1NdQOfcvjwPjc36aaNsUiPY21+ELZnwXA4XACATqd9o2I4HM6yD1ZNnjxj9ZqPYlZ/ePzn8/Ws+7yuqkru/r9G9J+LrbMY5CF5lczZGTsZ/3xTr5aH9F+NZLftFksWr5BKPV8WFgj4wrjtB5FBRnBwGIlEOvX7cetsGs2/r3N3794jI+NhVvaThjyrRlHt1649kgm9Xq/WqM1mtLVgMJhyucz6r1TqKZG4XLh42ro0o9FoMGAf+myFrBO6ubpHjZ+iVClLS4uFAkcAQOX/xncyWWVdC3mWk1VU9LJ79x6vP1RPMcFBoQCA8+d/t85svY4sEnqkq62rvPrfTgNR1q1b9/rUolyNyQhcvJlYT8FgNBpnzo4aPWpcULcQJydnAACfJ6DRaDwev7a2NiHhXPazJzqdLiU1adPm1cHBYSKRWC6XnTl7csjgkR4eXsjo4eixgz3Cenfs2MXX1//ylfOXL583mUwvCwvi4w9c/+vq4EEjkGFHQUHe25NnWF+64EX+9etXhELHsrLS2G2bi4pekgCIiIgikUhKZW3in5dksoraWkV5eamnp7dE4nr+/B+3km9YLCAzM33b9q8NRoN1PfN1BoNh5uyoysoKmazy1O/H9TrdvLnvC4WOCZfPZmVlenu3yy94vmXrepm8snPnbiEhPTOfpN+5k5xfkGs0GG4mXdu2/WuRo/ijD2OsvX7inwlqlSoyIopEItVVDJ8vkMkqzp47lZ+fq1Kr7t5N2fzV2r59B3I5XAaT9cfpE/kFz0mAlPkk3dfX//XyGn6J6ufpta4+DIETxoXk7NOJUKnU0JBeh4/ss+aay+Fu+36/t7fvovc/dHaWnDp1/M6dZJFI3C98kJPYuf6lubtJ47Yd2LUnNv7oARKJ5O8fMH5cnavjc2e/J5dVbo/bwuXyIsZETZ4Y/W3spvsP7nYPDhs2bHRWdmbC5XPJKX+NHBHZp0//fuGDvtwYe/DQ7h07v2GzOV27BHftWl9nrNFqgoPCrly9oFIpfXz8Nm2MRVZl1639+vttX61Yucjd3WPOrHc3fhljfcqggcPIFMqOXd9azOawsN7vLlzGZrOtj5pMJvL/bmZTTzH/t+wTFxe3s2dPJt267iR2DgvrTaVQkU/mow8/27d/R9yOrf7+AQMHDsMsr/Gwz0G9fUmu14JuAx0bviCTyYTcvcdisRSXFM1fMGXypOg5s9+1S5Wtg8FgmDFrvL9fwIb1W/GuBQAArsQXdx8k8ArEGIvYp7XQ6XTvL57l7OzSrWt3Bwdaevp9rVbbrl17uyy8qSmVyqnTIzAfWvjOBxFjxjf+JV6+LPjz2uWU1JtlZaVLFq1o/AKbmn1iQSKRhg8bk5h46eCh3TQazcfHb+2aza+sehEWi8Xau+co5kM8bn0XgWi4p1mZx3/5ydfXf+2azdYN/ERmt04EanHq6URa84516D+DsYAwwFhAGGAsIAwwFhAGGAsIA4wFhAHGAsIAYwFhgLGAMGDvE2GwKGYT9qFvUKvB5lGptDe50h5fTC3Jb/JTlyB85WUoxW7YRwVjx0Lqz9JrWvOdIqDKYq1nAIvOpGA+ih0LCpXUc6Rjwk9FTVwbhA+D3nzjROnASU51zVDfjSOKcjWXfioNGuAokNBZXHg/kRaPRALVlXplleHOxcqZq73q+U7/5TYzymrjvcSq0nyturaV9ykGg4FMJlMo2I1q68B1pJLJJHc/Ro8RovrnbOt3Tbb64osvOnfuPG7cOLwLIQTYNaAiIyOFQnifJRRsLSAMcCsn6sKFCw8fPsS7CqKAsUClpaXl5eXhXQVRwE4EVVJSwmQyBQJ4ezYAYwFhg50I6vfff7979y7eVRAFjAUqIyOjsLAQ7yqIAnYiqLy8PA6H4+RU526CNgXGAsIAOxHU8ePHU1JS8K6CKODGb9SzZ8/odDreVRAF7ERQWVlZPB7P1dUV70IIAcYCwgDHFqj4+PikpCS8qyAKOLZA5eXl2V77rI2DnQgKbrewBWMBYYBjC9SxY8eSk5PxroIo4NgClZuby2Q29KrFrR7sRFBPnz7l8Xhubm54F0IIMBYQBji2QF24cOH+/ft4V0EUMBaotLS0goICvKsgCjjkRA0cOBButLCCYwsIA+xEUPfu3cvPz8e7CqKAsUCdP3/+wYNmvSE7kcGxBapbt27u7u54V0EUcGwBYYCdCAqOLWzBWKDg2MIWHFugBg8eDLdbWMGxBYQBdiKoK1euZGRk4F0FUcBYoFJSUnJycvCugijg2AIFxxa24NgCwgA7EVRiYuLjx4/xroIoYCxQt27devbsGd5VEEVb70SmTJkCACCTyVqtlkqlkslkMplssViOHTuGd2l4autDTjKZnJ2dbTvFYrH07NkTv4oIoa13ImPGjGEwGLZT+Hz+vHnz8KuIENp6LCZMmODp6Wn912KxBAQEhISE4FoU/tp6LBgMxpgxY6wXPOHxeHPnzsW7KPy19VgAAMaPHy+VSpG/O3XqFBoaindF+IOxACwWKyIigkqlikSiOXPm4F0OIdh5TcRitiirjYCEfb88who9fMKZU1d8fHza+3arrTLiXc6bsVgsPEcH+y7Tbtst8jJUD29UF+ZoRK50nbqV36qIUIQutKJn6nbdOD1HOgqcsO9U+KbsE4vM24qsO8qwUWK+yD5lQW/EZLRUV+iu/VIaMc9V7G6H6wXaIRaPkxXP05UD34bneuPv5Pf5kQvdHCWN/XE2dsip15mz79fCTBDEoKmuty/KG7+cxsZCVqwzaNv0XhVCETrTcx4oG7+cxsZCITe6eMOLyBCITxeOrETXyIU0NhYmg0WjgusdBFJdrgegsRsI4OYsCAOMBYQBxgLCAGMBYYCxgDDAWEAYYCwgDDAWEAYYCwgDjAWEAcYCwgBjgcp8kqHTNXYPk8lkmjl7wo6d3/7rnJFvDdy1O7aRL9d0YCwAAODipTOLFs/WajWNXA6JROJwuK+cj9QS4X+yYWHhC6nUswEzNorFYiHVfeBx49sJBJlM3hl3yC6LwhcOsZDJKrfHbUlLS6U6OISE9Lxx4+qeXUd8fNoBAO4/uPvDvrjc3Gyh0DE4KGz+vEUikfhZTtaSpXM3b9q2d9/23NxsicR14YKlffsOQJZWUlq8c+e3afdSaTR6e/+AuXPfD+jQEQDw/bavrt+4uvzDmJ27vysqerl1y04Pqdf+gztTU5NUKqWHh9e0qXOGDhmJNBWx328GAIyLGgoAWPnx2pEjIusqpp73dfny+U2b1wAA2vsH7Nl9BJmYkHAu/tjB4uJCkUg8ZvT46dPmkMmvttBffrU2Kena7p2HkZ/HH6d//eXEkcrKchcXtyGDR749eUbz3865uTsRk8n06WfLHmc++uCDVVOnzLp+/UpQtxAkE2n3bn+8crG3l+/yj1ZPnhj96NG9D5e/q9VqkV/z5xtWTZwwLfbbvS4S1y82fVZTU40kbMnSuYramsWLli98Z6nBYPhg2fy8vFzktVQq5f6DO5d9sGrD+q3dg8OMJuPTp4/fGjvxvYXLeDz+xk0xT54+BgD07NF38qRoAMCXG2O3xe7r2aNv/cXUpVu3kA3rtwYEdLJOuXTp7JdfrfX3D1gds2nggGEHDu6KP3rwlWedOXsyIeHcqo8/RzJx6Me9e3/YNnjQ8BXL1wwcMPT4Lz99893Gpvkq6tPcrcWznKzsZ0/Xrtk8cMBQAMCLF/kXLp7W6/U0Gm173JbIiKilSz5G5gwN7TVrzsQ7d5NdXNwAAEsWrxg8aDgAYP78xQvfjX746F7/foMPH9knFDh+s2UXlUoFAAwbOjp65riz508tWbQcAKDX65d/GBMY2BlZoJur+6EDJ5CuZNSot8ZPGJqUdC0woJNQ6OjmJgUABAZ25vMFyMx1FdMvfFBdb83ZWeLsLLl46UxFeRnSbe07sKNLl6CYT78AAPTvN7i2VvHz8R8nRE1lsVjIU7KfPY3bsTV6+tzw8IEAgMrKivijB2I+2zig/xBkBpHI6bvYLxcvWs7j8prya3lVc8dCLqsEACBfAwBAKvU0m80ajVoulxUU5BUVvTx77pTt/OXlZUgsmAz00ECJxBX5BAEAqalJ5RVloyP6Wec3GAzIt4KcX2rNBCInN/vQj3uysjKRdksul2EWWVpaUlcxDX+nhYUvKisr3p48wzolLKz3+Qt/FBa9aO8fAABQKms//3wljUabOWMBMkNaWqrRaNy4KWbjphhkCnJcfmVFeSuPBfIdp6c/QD6aJ08yxGInPl9QXFwIAJg1853+/Qbbzu/oKC4pLbKd4kB1AACYzSYAgLxK1rt3v3fmL7Gdgc3mIH8wmSzb6ffu31m5aklwUOjHK9ayWew161aYLWbMIquqZHUV0/B3qlQpAQACgaN1CpfLQ75j5L1fvHTG09NbXaY+c+a3qKgpAACZvBIAsGljrLOTxHZR1l9Rs2nuWPj6+oWF9tr7w7ayspLqmqqkW9djPtsIAOBwuAAAnU7r6end8KVxubyamuoGPuXw4X1ubtJNG2ORHsfa/FhZT5n5b8W8AvlqkTEQoqpKbg0H8gv57ps9Px3+4eCh3YMHjxAIhNaHGvO6doHDdosli1dIpZ4vCwsEfGHc9oPIIEMq9ZRIXC5cPK3RoBsPjEajwWCof1Hdu/fIyHiYlf3EOsX69NfVKKr92rVHMqHX69UatdmMthZIRJCO6T8X8wqRSOwicb19O8k65fr1KwwGw8+vA/JveN+BAoFw9ux3yRTKvv07AADBwWEkEunU78cb8naaFGXdunWNeX5FoU4hN3p0aOhN641G48zZUaNHjQvqFuLk5AwA4PMENBqNRCJJJK7nz/9xK/mGxQIyM9O3bf/aYDR07NhFLpedOXtyyOCRHh5eyOjh6LGDPcJ6d+zYxdfX//KV85cvnzeZTC8LC+LjD1z/6+rgQSOQYUdBQZ5t117wIv/69StCoWNZWWnsts1FRS9JAERERJFIJAaT9cfpE/kFz0mAlPkkPaBDx7qK+dc3mPhnglqlioyIAgBwObzjJ45UVJQZDIaTp36+cvXC9Glzw0J7AQCO/XzI3z8gLLQXnU5nsdhH4vf37Bnu6+NXW1ubkHAu+9kTnU6Xkpq0afPq4OCw+leMX5F1t8YviMPiUhr+lNc1dydCpVJDQ3odPrLPaERPDOdyuNu+3+/t7dsvfNCXG2MPHtq9Y+c3bDana5fgrl271780dzdp3LYDu/bExh89QCKR/P0Dxo97u66Z585+Ty6r3B63hcvlRYyJmjwx+tvYTfcf3O0eHObuJv3ow8/27d8Rt2Orv3/A2MgJ/6EYhMlkIlPQr2TEiAitTnvi1/iEy+fEIqd3FiyZ8vbM158SGRF19uzJ7XFb4rYdWPT+h87OklOnjt+5kywSifuFD3ISOzfkde2rseegZqYoXj7T9hn7BqWbTCYKhYL05cUlRfMXTJk8KXrO7HcbUwZBGAyGGbPG+/sFbFi/Fa8aTu96MXKWi8i1UaehNndrodfr31s009nZpVvX7g4OtPT0+1qttl279s1cxn+jVCqnTo/AfGhEinXwAAATR0lEQVTSxGgAQErqzbKy0iWLVjR7aXbW3LEgkUjDh41JTLx08NBuGo3m4+O3ds3mV9YDCYvFYu3dcxTzobS027t2f+vr6792zWbrhvmWC4dOBGpSdulE4I51CAOMBYQBxgLCAGMBYYCxgDDAWEAYYCwgDDAWEAYYCwgDjAWEobGxoFIBk9OoXfuQfQkltMZfiL+xseA70Ypz1Y0uA7IPs9mSl650bNwOETvEwklKozFgT0QU8lKdf3dO45fT2G+UTCF36cu7fLioAfNCTe5qfHHfyDc4wq8u9rlxREGmKuWCPHSkWOBEp9Fh49Hc1LXG6gr99V9Kp3zswRPa4ZYzdrvNTEm+5n5i9ctsNYtDbYmXezZbzACQyC3tvkkAALE7vbpc79uF3WuMiMGyz/Df/ndN1qpN9ZwbTlhbt24NDAwcM2YM3oW8OQugs+zcQtv/oD17BbaZWUh6MtVEZ8IeEMDNWRA2GAsUn8+n0eCN1lAwFqiamhq9Xo93FUQBY4FydHRs/ovOEBaMBUoul9vrClqtAIwFCrYWtmAsULC1sAVjgaLRaK9fA6/Ngh8ESq/XW6+CAsFYQBhgLFCOjo6t4KLM9gJjgZLL5fVfjbVNgbGAMMBYoLhcroODHQ5gaR1gLFC1tbVveonFVgzGAsIAY4Gi0+kUSos8gKgpwFigdDqdydTyDkFtIjAWEAYYCwgDjAWEAcYCRafT4R5UK/hBoHQ6HdyDagVjAWGAsUDBEwJswVig4AkBtmAsIAwwFih45LctGAsUPPLbFowFhAHGAgVPCLAFPwgUPCHAFowFCg45bcFYoOCQ0xaMBYrD4cBDfK1gLFBKpRIe4msFY4Fis9lUanPfFZawYCxQKpXKett3CMYCBddEbMFYoOCaiC37X8W3ZYmKiiooKEBu/g4AsFgsFoslMDAwPj4e79Lw1NZbi8GDB5PJZOvVqEkkEofDmTNnDt514aytx2LSpEmenp62U9q1azd06FD8KiKEth4LiUQyaNAg6798Pj86OhrXigihrccCaTC8vLyQv/38/IYMGYJ3RfiDsfi7weDz+VOnTsW7HEKAsQBIgyGVSn18fAYOHIh3LYSA5wrq83RlZkqtRmWqKsP/kGujyUQikSh4H4njQCc50Mku3szQoQKBE24nKOAWi/vXqotztd6dOSI3hgMNNlooEgmoFMaaCn3aFdnwaImrDz4X/8MnFrfOymqrTH3GOjf/S7cgF/YX9hgh9O7Ebv6XxuFnWpKnUcgMMBP/auRc97tXq8wmHH63OMSiKEfD4MADXv4diUSymEFpPg5XC8UhFhql2ckDXi+3QVx9WdXlOIzHcYiFssaIS8PYEuk0Zr2+bXQiEPHBWEAYYCwgDDAWEAYYCwgDjAWEAcYCwgBjAWGAsYAwwFhAGGAsIAwwFhCGFhCLZzlZg4aEJif/ZZelZT7JaPhJhVu2blj47r+fHxCz5qOGzNaCtIBY2NHFS2cWLZ6t1WoaOD+TyWKxcDg4Cndt64oOb3ry8eJFHzVZLYTWYmKReC1h997vS0uL/fw6LFywtGvXYGS6Vqvdt3/H1cSLer3OQ+o1efKMwYOGAwBeviz4LvbLJ08zuFxer57hyz5YlXD5XOz3mwEA46KGAgBWfrx25IjIul6uvLzs7aljkL/PnbnBYrGQDmj3ntisrEwGg9mnd//33vs/Hpf3yhMvXDz99Zb1q2M2IWXcf3D3h31xubnZQqFjcFDY/HmLRCJxU35O9tFiOpH8vNyJE6bNnrWwrKzkoxXvZWamAwDMZvNnMf+XnHxj+rQ5/7fsUz+/Dhu++PT8hT8AAFu+2fA8L2fR+x9NnDCtorKcTCb37NF38qRoAMCXG2O3xe7r2aNvPS/H5ws2rN86auTYvwvIf/7R8ncNBsPHK9bOmrHg5s0/P/985SvPysnJ/n7bV5MmTkcykXbv9scrF3t7+S7/aPXkidGPHt37cPm7LeJyCS2mtZg7573evfsBAIYNHT177sR9+3d8+83uG38lPkq/fyz+jFjsBAAYOmSkRqP+7eSx0aPeKi0tbu8fEDFmPAAASYNQ6OjmJgUABAZ25vMF9b8cnU4P7zuwtLTYOuVI/H4ymfz1V3FcDhcAwOXyNm1e8/DhvW7duiMzKJXKdetXBgR0emfBEmTK9rgtkRFRS5d8jPwbGtpr1pyJ6RkPQkN6NtnnZB8tJhZWYrFTeN9BV65eMBqNKSk3jUbjtOi/f9Mmk4nN5iDpOXrs0LbtX8+Ini8UOjb+dR88TAsODkMyAQAIC+sNAMjKzrTGYsvW9UVFLz/9ZANyDa7S0pKCgryiopdnz52yXU5VlbzxxTS1lhcLAICTk7PJZNJqtVVVMpFI/O3W3baPUqhUAMD8eYuEQscj8QcuXDz9zoKl48dNbuSLqlRKAV9o/ZfL5QEAKisrkH9zcrNLSoudnSXHjh3asH4rAKCqSgYAmDXznf79BtsuRyxuAWdCtMhYVFXJGQwGm83mcnnV1VUSievrl70ikUgTJ0wbNfKt72I3bdv+tV+79l26BCEP/bczpsRiZ4WixrYGAADnf42Hg4PDpi++k8kr132+8m5aamhIT+QhnU7r6endiPeKjxYz5LTSarUpqTeDgkJJJFL37j1MJtPpM79aH9Vo0G0SyMiOzWbPnv0uACD72VMAAJPBtP2Jv5FOnbo+eJim1aJnbdy4cRUAYI2al6dP587dBvQfEhwUuj1ui9FolEo9JRKXCxdPW0syGo0t5dKfLaa12Hdgh7xKplarLl46o1DUzJ61EBlAnDl7cvee70tKi9v7B+TkZN9M+vPQgV8ZDMa69Ss5bE5oSK+U1JsAgA7tAwEAnTp3o1AocTu3jhoxVqfXjY2c0PACoqfNTUy8tPKTJZERE8rLS3/8aW9wUGhQt5BXZlu8aPmChdNO/X580sTpi97/aM3aFYuWzB4bOdFsMl1KODts2OiJE6bZ+7OxP8q6deua+SVzHih5YlrDT8eWy2UPH93r32/wyVM/Jyf/5eYm/XTV+o6BnQEAFApl4IBhSqXi2rXLN/5KVKmVo0a+1aVLEJlMLi4uTEm9eTXxokareWfBkvDwgQAAHpfn5CS5du1ycvJftbWKESMi6n/p9IwHd9NSZ0TPo1KpPB6/S+fgO3eTz5z9LSv7yaCBw1csX4N0Xol/JqhVqsiIKGR9p6am6uSpn0eOGBvQoWNAh46PHt1PuHzuydOMdr7+w4aNeaPtFkU5ahaH7OLd3Gdb4XBq8oVDpdIOHO+OnGZ+3f9gwxef3rmTfPqPP/Eq4PbFSpELNWjAv6xO212L6USawtJl8/Pycl6f3qNHX3+/Dk+fPr5+4+qkidPxKA1nbToWa2K+NBgxxoBqlfK9RbPc3T3mzX1/ytsz8SgNZ206Fsi2UUwJF5ObtxZiaXkrqFAzgLGAMMBYQBhgLCAMMBYQBhgLCAOMBYQBxgLCAGMBYcAhFgwWhdKmN66+ATqDTKGSmv91cYgFnUmqLm8ZR6PgrrJYyxXi8BvCIRZOUrpeA+842iAkEhBKcLjiMQ6x8A/mVpXpX2Spmv+lW5Y7lyolnnS+CIfbR+BzhwCz2XJqR5FvV167blzrXQUhK4PenHa5kiOg9B4twqUAPG8zc/3XivRbNe5+LJMR/2s9m81mEomEe0apFFAjN1CppE59+M1/UJYV/rfHrSjS6TVmfGsAABw+fNjHxyc8PBzvQgBHQOUKqWQKngHFf03RyZ0QdzY3OpTTBc7ufky8CyEEuDkLwgBjgaLRaGS8719HHPCDQOn1erMZ/yEOQcBYoAQCwesnsrZZMBao6urqFnFBkuYBY4Hi8Xg0Gm63oyUaGAuUQqHQ6/G/eTNBwFhAGGAsUHQ6nUKh4F0FUcBYoHQ6nclkwrsKooCxQAkEAjjktIKxQFVXV8MhpxWMBYQBxgLl6OgIt3JawVig5HI53MppBWMBYYCxQPH5fLgmYgVjgaqpqYFrIlYwFhAGGAsUm81GruwPwVj8TaVSGY3wXDcUjAWEAcYCBQ/xtQU/CBQ8xNcWjAUKtha24AeBgq2FLRgLCAOMBQqeJ2ILxgIFzxOxBWMBYYCxQME9qLZgLFBwD6otGAsUbC1swVigYGthC8YCwgBjAWGAsYAwwFhAGGAsUFwu18EBh6trExOMBaq2ttZggPctQOF/FV98DR8+XC6XvzLR09Pz5MmTOFVECG29tejVq9crPww6nT5t2jT8KiKEth6L6dOnSyQS2ykeHh4TJkzAryJCaOux6NChQ1hYmLXBoNPpkydPxv0+Abhr67EAAEybNs3aYLi5uUVFReFdEf5gLECHDh1CQkIsFguNRps6dSre5RACjAUAAMyaNcvFxcXd3R02FYiWt4JqMVueZygriw3KKqNKYSJRgFZphyO2i0uKmUymUCBs/KK4QqrRYGHzKXwxVeLJcG/X8u5R0pJi8ex+bcat2qIctaOUQ6FRqXSKA41CoRHxYppGndGoMxmNZm2NRqMweAWygwbwXH1aTD5aRizyM1U3TskYPAaDz+Q5sfAu582YDGZFhUpZoeTwyAMniIWSFnCwD9FjYbGAcwfL5KVGZz9HBrcFfKD1UJSrKnKr2odw+r2Fz/0KG47QsTDqzT9tfCFuJ2pxLUQ9KvKq6FTD2Hdc8S6kPsSNhUFvOrzxpbSrC43V2nZs1pQqgV791kLiJoO4K6h7P8nz7iFtfZkAAPBdOIDO/iW2CO9C6kTQWMR/9dI3zI1MbrUbofkSNpXFTPylAu9CsBExFsnnZFwnDpPfys8IdfTgV1Vach8p8S4EA+FioaoxZtxS8Fx5eBfSHLgS3o2TlXhXgYFwsbhxqtLJ1xHvKpoJjeXAFDDTk2rwLuRVxIqFQmaQl5sEbhy8C8GQeveP5at7KhR2/nGLvASZqYTrR4gVi+cZSgqtFa561MOBQdUoTeWFWrwL+QdixeLZAzVH3Hq2XDUQS8TKfaDCu4p/INB1a5GLV3FETbI/Sa/XXriy6/6jSwaDzknsNTB8elCXYQCAG7eOPUi/0r/P1AtXdtXWVrq7BUx66xNnJ2/kWUXFWb+f//ZlUSaPK3YSeTZFYQAArhNLVkqs4QWBYqFRmJRVBkkD5nxTZrP5QPxHVVUlg/vP4nAcc5+nHfklRqfX9AwZCwB4UZhxPSl+0lufmkzGX09/+fPJ9UsXHgAAlFXk7zrwHpslGD3sfQqZevna/iYoDQAAHGjUwufE6kQIFAt1rdGB0SR7ydMz/8zLf/DpR7/zeU4AgO5dR+j06pvJx5FYAADmTN/K44oAAOG9Jp+5+L1KXcNm8c9d2k4ikZcs3M9hCwEAJDL55Jmvm6I8Kp2i05gsZguJMJvviBULOrtJxptPspJMZuOmb8dbp5jNJibj7/UdOg3tuYQCVwCAQlHhQKVn5aT0DpuAZAIAQCE34WfFE9OVNUaukCjDbQLFgkwhG3RNcifSWqWMxxW/O2fHP14O62umUhyQ0ChqK00mo6OwmfZmaWoNVBqBhv8EigWbSzE1TSxYTJ5SVSUUuDo4NHSDOtJIKJVVTVHPKyxmi9FgZrIJdJgZgRLK4lH12iaJhV+7MLPZdOv2b9YpOr2m/qcwGGyxyOPh46tGY5OfmGrQmZhsAv0+idVacARUOpNsNpnJFDuHNaTbqNS7v5+9tL2qusTdtUNx6bP0zGsfLz1OozHqedbwQfOP/rp2+975PbpHkMjkv5KP27cqK71a7+JdXyXNj0CxAAA4edAVZWq7b/ymUh0WzNp2PmHH/UcJyXdOOYk8+/SIolD+5b137zZSo6m9lhR/NmG7xMnXy6NzRWWBfQtDKCvUnXoQKxbEOjor56EyNUHh3rkpNl4QV/ZfL6av8mDzCPQTJVApAIB2XdipF+sb5VksltWbhmI+xGEJlOrq16d3Cug/dcJae1Wo0So3fvMW5kNeHl0KXqa/Pt1J5PnBuwfrWqC6WuvWjkmoTBCutQAA3E6Q52WZJH517luXVxVjTjcaDVQqxno/jca0bntoPLPZXF1Tiv2YhQRIGB8mheKAbEbDlH+3eES0mGinkBArpACAHsMd7yc+F3nxqQ7YK2yOQrdmL+pvZDLZjgUoylV8EYVomSDWCqrVoLedqgsxuoPWR1VZO2RKnQ0JjogYi/bBXIkbWfailSej8FFpr5F8niNRNnjbImIsAAD9xonpFENlPrF2N9tR0eOKwFCWTyciHodGxCGnrXMHyrR6qshLgHchdlaUUR7cn9OxJxfvQupE6FgAAK7/VllebBZ5C8lUgjZsb0Sr1Bc/Lu89xjEwjLiZaAGxAABk3a1NPF4u8uI7t7PbembzM+pMZTkyo0Yf+Y6Lo4Top8C0gFggks/Lcx6qyFQHrhOL68xqKVc9M+pNinK1slJl0hl6jnLs2LNlnP/SYmIBADDozc/uK7PSVJVFWjKVTKVRqDSKA5NmMjbJftf/jOpA0an0Rr2RBIBWZfAMYAeEcHw6s/Gu6w20pFhYWSwWealeXWtSKYxGncVoJNZbcKCTaXQSi0dlcSlC5xZ5TY4WGQuoqbWG4T1kdzAWEAYYCwgDjAWEAcYCwgBjAWH4fyUV8UgpF+MCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Construct the graph: here we put everything together to construct our graph\n",
    "graph = StateGraph(OverallState)\n",
    "\n",
    "# Node to generate subjects related to the topic\n",
    "graph.add_node(\"generate_subjects\", generate_subjects)\n",
    "\n",
    "# Node to generate a joke about a subject\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "\n",
    "# Node to select the best joke\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "\n",
    "graph.add_edge(START, \"generate_subjects\")\n",
    "\n",
    "# PAY ATTENTION HERE: see how we use the continue_to_jokes function we created with Send\n",
    "graph.add_conditional_edges(\"generate_subjects\", continue_to_jokes, [\"generate_joke\"])\n",
    "\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cebb41-fa5e-4e1b-a27a-90a5ed32077b",
   "metadata": {},
   "source": [
    "## Let's review what is going on under the hood in this app\n",
    "The app defines a **map-reduce workflow** in LangGraph to generate and evaluate jokes based on a given topic. Here's a simplified explanation broken down step-by-step:\n",
    "\n",
    "#### Purpose of the Code\n",
    "The code builds a **graph-based workflow** to:\n",
    "1. Generate a list of subtopics related to a given topic (**map** step).  \n",
    "2. Create a joke for each subtopic (**map** step).  \n",
    "3. Select the best joke from the generated ones (**reduce** step).  \n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **Inputs and Outputs**:\n",
    "   - **Input**: A topic provided by the user (e.g., \"Technology\").  \n",
    "   - **Output**: The best joke related to that topic.\n",
    "\n",
    "2. **Model Setup**:\n",
    "   - Uses the selected LLM (in our example chatGPT4) to generate text outputs based on prompts.\n",
    "\n",
    "3. **Data Models**:\n",
    "   - **Subjects**: Holds the list of subtopics generated.  \n",
    "   - **Joke**: Stores a joke for a specific subtopic.  \n",
    "   - **BestJoke**: Keeps track of the ID of the best joke selected.\n",
    "\n",
    "4. **State Management**:\n",
    "   - **OverallState**: Tracks the topic, generated subjects, jokes, and the best-selected joke.  \n",
    "   - **JokeState**: Tracks the specific subject while generating a joke.\n",
    "\n",
    "#### Workflow Breakdown\n",
    "\n",
    "1. **Subjects Generation** (Mapping Step):\n",
    "   - **Function**: `generate_subjects()`\n",
    "   - Generates 3 subtopics based on the main topic using the AI model.\n",
    "\n",
    "2. **Joke Generation** (Mapping Step):\n",
    "   - **Function**: `generate_joke()`\n",
    "   - Creates a joke for each subtopic.  \n",
    "   - Uses parallel processing, meaning all jokes are generated simultaneously.\n",
    "\n",
    "3. **Dynamic Mapping**:\n",
    "   - **Function**: `continue_to_jokes()`\n",
    "   - Dynamically maps each generated subtopic to the joke creation node, even if the number of subtopics is unknown beforehand.\n",
    "\n",
    "4. **Best Joke Selection** (Reduce Step):\n",
    "   - **Function**: `best_joke()`\n",
    "   - Combines all generated jokes and evaluates them to find the best one.  \n",
    "\n",
    "#### Graph Construction\n",
    "\n",
    "- **Nodes**: Represent tasks like generating topics, creating jokes, and selecting the best joke.  \n",
    "- **Edges**: Define transitions between tasks.  \n",
    "- **Dynamic Mapping**: Allows variable-length inputs (e.g., multiple subtopics) to be handled without predefining their count.  \n",
    "\n",
    "#### Key Concepts in Action\n",
    "\n",
    "- **Map Phase**:  \n",
    "  - Breaks down the topic into subjects or subtopics (mapping) and processes each subtopic in parallel to generate jokes.\n",
    "\n",
    "- **Reduce Phase**:  \n",
    "  - Combines results (jokes) and selects the best one.\n",
    "\n",
    "- **Flexibility**:  \n",
    "  - Handles unknown numbers of subtopics dynamically using conditional edges.\n",
    "\n",
    "#### Real-World Analogy\n",
    "\n",
    "Imagine you want to prepare a comedy show based on a theme.  \n",
    "1. **Topic**: \"Technology.\"  \n",
    "2. **Step 1 (Map)**: Brainstorm related subjects like \"AI,\" \"Smartphones,\" and \"Robots.\"  \n",
    "3. **Step 2 (Map)**: Write jokes about each subject.  \n",
    "4. **Step 3 (Reduce)**: Select the funniest joke to tell at the show.\n",
    "\n",
    "#### Final Output\n",
    "\n",
    "The graph executes these steps, processes each stage, and outputs the **best joke** related to the given topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af3c49-9b45-42cd-a3d4-2c7cd4128391",
   "metadata": {},
   "source": [
    "## Let's pay attention to the line where we activate the Send Magic in the previous code\n",
    "In the previous code, it is important to understand the line where we are activating the \"Send Magic\":\n",
    "\n",
    "```python\n",
    "graph.add_conditional_edges(\"generate_subjects\", continue_to_jokes, [\"generate_joke\"])\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "Here's a breakdown of its components:\n",
    "\n",
    "1. **`graph.add_conditional_edges`**:\n",
    "   - This function defines edges in the state graph that are created dynamically based on some condition or computation. Unlike a static edge (e.g., `graph.add_edge`), conditional edges depend on the result of a user-defined function, which is evaluated at runtime.\n",
    "\n",
    "2. **`\"generate_subjects\"`**:\n",
    "   - This is the source node in the graph. After this node is processed, the graph evaluates the `continue_to_jokes` function to determine what happens next.\n",
    "\n",
    "3. **`continue_to_jokes`**:\n",
    "   - This is the custom function provided to determine the conditional edges. It takes the current `state` as input (an instance of `OverallState`) and returns a list of `Send` instructions.\n",
    "   - Each `Send` specifies the next node (\"generate_joke\") and the associated payload (data to pass to the node). Specifically, for every subject in the `state[\"subjects\"]` list, a `Send` is created that includes the subject as part of the payload.\n",
    "\n",
    "   Here's the `continue_to_jokes` function:\n",
    "   ```python\n",
    "   def continue_to_jokes(state: OverallState):\n",
    "       return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "   ```\n",
    "   - **`Send(\"generate_joke\", {\"subject\": s})`**:\n",
    "     This instructs the graph to send control to the `generate_joke` node, passing a dictionary `{\"subject\": s}` as the input data, where `s` is each subject in the `state[\"subjects\"]` list.\n",
    "\n",
    "4. **`[\"generate_joke\"]`**:\n",
    "   - **This is the list of potential target nodes for the conditional edges**. In this case, it's only one target node, `\"generate_joke\"`, where the function `generate_joke` will handle the payload sent by each `Send`.\n",
    "\n",
    "#### What Happens in Practice\n",
    "- After the `\"generate_subjects\"` node runs, it populates the state with a list of subjects (`state[\"subjects\"]`).\n",
    "- The `continue_to_jokes` function is invoked, creating a `Send` instruction for each subject in the list.\n",
    "- Each `Send` dynamically creates an edge leading to the `\"generate_joke\"` node, passing the corresponding subject as input.\n",
    "- **The graph then processes the `\"generate_joke\"` node multiple times—once for each subject—generating jokes for all the subjects**.\n",
    "\n",
    "#### Visual Analogy\n",
    "Imagine `\"generate_subjects\"` as a machine that spits out a list of subjects like `[\"cats\", \"dogs\", \"penguins\"]`. The `continue_to_jokes` function acts as a distributor, sending each subject to the `\"generate_joke\"` node separately. For example:\n",
    "- `\"cats\"` → `\"generate_joke\"`\n",
    "- `\"dogs\"` → `\"generate_joke\"`\n",
    "- `\"penguins\"` → `\"generate_joke\"`\n",
    "\n",
    "This ensures that each subject gets processed independently.\n",
    "\n",
    "#### Summary\n",
    "The line of code dynamically connects the `\"generate_subjects\"` node to the `\"generate_joke\"` node for every subject in the `state[\"subjects\"]` list, allowing jokes to be generated for each subject independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357b5a1-9849-4a3c-a9d7-3837aec0abaf",
   "metadata": {},
   "source": [
    "## And now we can run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad06095-df25-4fc5-b9f6-7126f9b8bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_subjects': {'subjects': ['Mammals', 'Reptiles', 'Birds']}}\n",
      "{'generate_joke': {'jokes': [\"Why don't mammals ever use computers? Because they're afraid of the mouse!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why don't reptiles like fast food?\\nBecause they can't catch it!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why do seagulls fly over the ocean? Because if they flew over the bay, they'd be called bagels!\"]}}\n",
      "{'best_joke': {'best_selected_joke': \"Why do seagulls fly over the ocean? Because if they flew over the bay, they'd be called bagels!\"}}\n"
     ]
    }
   ],
   "source": [
    "# Call the graph: here we call it to generate a list of jokes\n",
    "for s in app.stream({\"topic\": \"animals\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 022-map-reduce.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 022-map-reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
