{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "# Store\n",
    "* We'll introduce the LangGraph Memory Store as a way to save and retrieve long-term memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60feb5d9-128c-4fe0-8b1e-d547226bc124",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 025-store.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1361fb-6fe2-4df9-a172-ced26f1a64a0",
   "metadata": {},
   "source": [
    "## Let's build a chatbot that can remember facts about the user\n",
    "* We will build a chatbot that uses **both short-term (within-thread) and long-term (across-thread) memory**.\n",
    "* We will focus on **long-term semantic memory**, that can remember facts about the user. These long-term memories will be used to create a personalized chatbot that can remember facts about the user.\n",
    "* The app will **save memory \"in the hot path\"**, as the user is chatting with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311da77c-09b3-4904-bfd0-1ac2cfee0f40",
   "metadata": {},
   "source": [
    "## We will make use of LangGraph's Memory Store (aka Store) to build the long-term memory of our chatbot\n",
    "Long-term memory:\n",
    "* Scope: across sessions / threads.\n",
    "* Use case: remember info about the user across sessions.\n",
    "* The LangGraph Memory Store provides a way to store and retrieve information across threads in LangGraph.\n",
    "* It uses an  [open source class](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/) for persistent `key-value` stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267d4e63-4f41-4ad0-a839-7b05c58f1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a2545-5f6e-4d17-841c-4f5677160fe9",
   "metadata": {},
   "source": [
    "When storing objects in the Store, we provide:\n",
    "* The namespace (similar to directory name).\n",
    "* The key (similar to file name).\n",
    "* The value (similar to file content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4222b97-c66b-4649-81a1-dab2ef05a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "key = str(uuid.uuid4())\n",
    "\n",
    "# The value needs to be a dictionary  \n",
    "value = {\"food_preference\" : \"I like pizza\"}\n",
    "\n",
    "# Save\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8c7cb-11ef-4946-972a-02b51b0d11c5",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "This code demonstrates how to store data in memory using **LangGraph's InMemoryStore**. Here's a simplified explanation step-by-step:\n",
    "\n",
    "#### Import required modules\n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "```\n",
    "- **uuid**: Generates unique identifiers (useful for creating unique keys).  \n",
    "- **InMemoryStore**: A tool provided by LangGraph to store key-value pairs in memory.\n",
    "\n",
    "#### Create an in-memory storage object\n",
    "```python\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- Creates an instance of **InMemoryStore** to store data temporarily (not saved to a file or database).\n",
    "\n",
    "#### Define identifiers\n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "```\n",
    "- **user_id**: Represents the ID of the user (in this case, \"1\").  \n",
    "- **namespace_for_memory**: A tuple combining the user ID and the category (\"memories\"). It helps organize data for each user and context.\n",
    "\n",
    "#### Generate a unique key\n",
    "```python\n",
    "key = str(uuid.uuid4())\n",
    "```\n",
    "- **uuid.uuid4()**: Creates a random unique identifier.  \n",
    "- **str()**: Converts it to a string so it can be used as a key.\n",
    "\n",
    "#### Prepare data to store\n",
    "```python\n",
    "value = {\"food_preference\" : \"I like pizza\"}\n",
    "```\n",
    "- **value**: A dictionary storing some information (user's food preference in this case).\n",
    "\n",
    "#### Store the data\n",
    "```python\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- Saves the dictionary (**value**) in memory under the **namespace_for_memory** and the generated **key**.  \n",
    "- This means the data is stored in a specific \"folder\" (namespace) for the user and can be retrieved later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39704d0e-a930-4d2d-a616-a50ebedf926d",
   "metadata": {},
   "source": [
    "## How to retrieve memories from the Memory Store\n",
    "* **We use `search`** to retrieve objects from the store by namespace.\n",
    "* This returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e72a12-54a3-4739-ab3e-ea022f551838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'food_preference': 'I like pizza'},\n",
       " 'key': '4803d5a1-5c53-4f91-969e-b507b5b197c5',\n",
       " 'namespace': ['1', 'memories'],\n",
       " 'created_at': '2025-01-30T15:49:26.755675+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:26.755677+00:00',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efc8d29-902c-4eab-8a7b-a97e365fcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4803d5a1-5c53-4f91-969e-b507b5b197c5', {'food_preference': 'I like pizza'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories[0].key, memories[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc46b16-dda8-4ccb-984c-89c4a5331746",
   "metadata": {},
   "source": [
    "* **We can also use `get`** to retrieve an object by namespace and key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a862c23-d41e-4e73-95f2-954a21d7272a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'food_preference': 'I like pizza'},\n",
       " 'key': '4803d5a1-5c53-4f91-969e-b507b5b197c5',\n",
       " 'namespace': ['1', 'memories'],\n",
       " 'created_at': '2025-01-30T15:49:26.755675+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:26.755677+00:00'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = in_memory_store.get(namespace_for_memory, key)\n",
    "memory.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da64e68-52a7-4ded-b2d3-577e6349d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4803d5a1-5c53-4f91-969e-b507b5b197c5', {'food_preference': 'I like pizza'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.key, memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075cb45-8fd8-4768-92ef-0457a4883623",
   "metadata": {},
   "source": [
    "## OK. Now that we know more about the Memory Store, let's start building the chatbot with long-term memory\n",
    "\n",
    "We want a chatbot that has two types of memory:\n",
    "\n",
    "1. `Short-term (within-thread) memory`: Chatbot can persist conversational history and / or allow interruptions in a chat session.\n",
    "2. `Long-term (cross-thread) memory`: Chatbot can remember information about a specific user *across all chat sessions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a256f-61b9-4a52-87d8-4b236f5bc111",
   "metadata": {},
   "source": [
    "* **For short-term memory we'll use a checkpointer**.\n",
    "* And **for long-term memory we'll use the LangGraph Memory Store**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d732d092-df52-4c34-a452-4adda05e0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat model \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb286232-2fd5-4123-830b-9b3b5b2e0369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # Extract the actual memory content if it exists and add a prefix\n",
    "    if existing_memory:\n",
    "        # Value is a dictionary with a memory key\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "    \n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Extract the memory\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # Overwrite the existing memory in the store \n",
    "    key = \"user_memory\"\n",
    "\n",
    "    # Write value as a dictionary with a memory key\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75ecdc-aa4f-403c-af11-93e6859e2613",
   "metadata": {},
   "source": [
    "## Let's explain in simple terms the first relevant function of the previous code: call_model\n",
    "\n",
    "The function called **`call_model`** generates a **personalized chatbot response** by using stored memory about the user. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Function Purpose\n",
    "The function:\n",
    "- Loads **memory** about the user from storage.  \n",
    "- **Personalizes** the chatbot's response based on the stored information and the current chat messages.\n",
    "\n",
    "#### Input Parameters\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **`state`**: Contains the current chat messages.  \n",
    "- **`config`**: Provides configuration details, like the user ID.  \n",
    "- **`store`**: Handles the memory storage system (where user information is saved).\n",
    "\n",
    "#### Retrieve User ID\n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "```\n",
    "- Gets the **user ID** from the configuration.  \n",
    "- This ID helps identify the specific user's memory in storage.\n",
    "\n",
    "#### Fetch Memory from Storage\n",
    "```python\n",
    "namespace = (\"memory\", user_id)\n",
    "key = \"user_memory\"\n",
    "existing_memory = store.get(namespace, key)\n",
    "```\n",
    "- **Namespace** groups memory data under \"memory\" and the user ID.  \n",
    "- **Key** specifies what data to retrieve (e.g., \"user_memory\").  \n",
    "- **`store.get()`** fetches the memory data for this user.\n",
    "\n",
    "#### Check If Memory Exists\n",
    "```python\n",
    "if existing_memory:\n",
    "    existing_memory_content = existing_memory.value.get('memory')\n",
    "else:\n",
    "    existing_memory_content = \"No existing memory found.\"\n",
    "```\n",
    "- If memory exists, it extracts the content from storage.  \n",
    "- Otherwise, it assumes no memory is available.\n",
    "\n",
    "#### Create System Prompt with Memory\n",
    "```python\n",
    "system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "```\n",
    "- Prepares a **system message** (instruction for the chatbot) using the memory content.  \n",
    "- Example: *\"You are a helpful assistant. The user likes pizza.\"*\n",
    "\n",
    "#### Generate Response\n",
    "```python\n",
    "response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "```\n",
    "- **Combines** the system message (memory) and the user's chat history.  \n",
    "- Uses the **GPT-4o model** to generate a **personalized response**.\n",
    "\n",
    "#### Return the Response\n",
    "```python\n",
    "return {\"messages\": response}\n",
    "```\n",
    "- Packages the response as a dictionary to send it back.\n",
    "\n",
    "#### Example Walkthrough\n",
    "\n",
    "**Stored Memory:**  \n",
    "- User ID: 123  \n",
    "- Memory: *\"Name: John. Likes hiking.\"*\n",
    "\n",
    "**Chat Input (New Message):**  \n",
    "> \"I’m planning a trip next weekend.\"\n",
    "\n",
    "**Process:**\n",
    "1. Fetch memory: *\"Name: John. Likes hiking.\"*  \n",
    "2. Create prompt: *\"You are a helpful assistant. The user likes hiking.\"*  \n",
    "3. Generate response using memory + input.\n",
    "\n",
    "**Chatbot Output:**  \n",
    "> \"That sounds exciting, John! Are you planning to hike somewhere special?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9cbaf6-78e1-49e3-8436-d62a214fe514",
   "metadata": {},
   "source": [
    "## OK. Now let's explain in simple terms the other relevant function in the previous code: write_memory\n",
    "\n",
    "The function **`write_memory`** **updates and saves user memory** based on the chat history. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Function Purpose\n",
    "The function:\n",
    "- **Analyzes the chat history** to identify new information about the user.  \n",
    "- **Updates or overwrites memory** with the new information.  \n",
    "- **Stores the updated memory** for future conversations.\n",
    "\n",
    "#### Input Parameters\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "```\n",
    "- **`state`**: Contains the current chat messages.  \n",
    "- **`config`**: Provides settings like the user ID.  \n",
    "- **`store`**: Manages where the memory is stored.\n",
    "\n",
    "#### Get the User ID\n",
    "```python\n",
    "user_id = config[\"configurable\"][\"user_id\"]\n",
    "```\n",
    "- Retrieves the **user ID** from the configuration.  \n",
    "- This ID is used to locate the user's memory in storage.\n",
    "\n",
    "#### Fetch Existing Memory\n",
    "```python\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = store.get(namespace, \"user_memory\")\n",
    "```\n",
    "- Defines a **namespace** (folder-like structure) based on the user ID.  \n",
    "- Tries to **fetch memory** labeled as **\"user_memory\"** from storage.\n",
    "\n",
    "#### Check If Memory Exists\n",
    "```python\n",
    "if existing_memory:\n",
    "    existing_memory_content = existing_memory.value.get('memory')\n",
    "else:\n",
    "    existing_memory_content = \"No existing memory found.\"\n",
    "```\n",
    "- If memory exists, it extracts the content.  \n",
    "- Otherwise, it assumes **no prior memory** is available.\n",
    "\n",
    "#### Analyze Chat and Generate Updated Memory\n",
    "```python\n",
    "system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "```\n",
    "- **Creates instructions** (from `CREATE_MEMORY_INSTRUCTION`) for the chatbot to **extract new information** about the user.  \n",
    "- Combines:\n",
    "  - Existing memory.  \n",
    "  - Current chat history.  \n",
    "- Uses the **GPT-4o model** to **analyze and generate updated memory**.\n",
    "\n",
    "#### Save Updated Memory\n",
    "```python\n",
    "key = \"user_memory\"\n",
    "store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "```\n",
    "- **Saves the new memory** to storage, replacing the old one.  \n",
    "- Memory is stored as a **dictionary** with the key **\"memory\"** for easy retrieval later.\n",
    "\n",
    "#### Example Walkthrough\n",
    "\n",
    "**Initial Memory:**  \n",
    "- Stored: *\"Name: John. Likes hiking.\"*\n",
    "\n",
    "**Chat Input:**  \n",
    "> \"I also enjoy biking and recently visited Spain.\"\n",
    "\n",
    "**Process:**\n",
    "1. Fetch memory: *\"Name: John. Likes hiking.\"*  \n",
    "2. Analyze chat history: Adds *\"Enjoys biking, visited Spain.\"*  \n",
    "3. Updated memory: \n",
    "   ```\n",
    "   - Name: John  \n",
    "   - Likes hiking  \n",
    "   - Enjoys biking  \n",
    "   - Visited Spain\n",
    "   ```\n",
    "4. Save this updated memory.\n",
    "\n",
    "**Result (New Memory):**\n",
    "- The chatbot will remember John also enjoys biking and has been to Spain in future chats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed83aac-4d74-4d44-9ffb-9451af432686",
   "metadata": {},
   "source": [
    "## Finally, let's explain the all the previous code (including the two functions) in simple terms\n",
    "\n",
    "This code defines a **memory-enabled chatbot**. Here's a step-by-step explanation in simple terms:\n",
    "\n",
    "#### Import Required Tools\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "- **ChatOpenAI** initializes the GPT-4o model to process user inputs and generate responses.\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "```\n",
    "- **StateGraph**: Allows defining the chatbot's workflow using nodes and edges (steps and connections).  \n",
    "- **MemorySaver** and **BaseStore**: Handle short-term and long-term memory storage.\n",
    "\n",
    "#### Set Up the Chat Model\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **model**: Uses GPT-4o with zero randomness (**temperature=0**) for consistent responses.\n",
    "\n",
    "#### Define Chatbot Instructions\n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "```\n",
    "- Provides system instructions for the chatbot.  \n",
    "- Includes **memory** about the user to make responses personalized.\n",
    "\n",
    "```python\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "```\n",
    "- Specifies **how to update memory** based on the chat history, such as preferences, goals, or personal details.  \n",
    "- Ensures updates are factual and organized as a **bulleted list**.\n",
    "\n",
    "#### Process Messages and Memory\n",
    "\n",
    "**Step 1: Retrieve Memory and Generate Response**\n",
    "```python\n",
    "def call_model(state, config, store):\n",
    "```\n",
    "- **Purpose**: Fetches existing memory and uses it to personalize the chatbot’s response.\n",
    "\n",
    "**Key steps:**\n",
    "1. **Get user ID** from the input configuration.  \n",
    "2. **Retrieve memory** for the user from the store.  \n",
    "3. Use memory and chat history to generate a response using the **GPT-4o model**.  \n",
    "4. Return the updated messages.\n",
    "\n",
    "**Step 2: Update Memory**\n",
    "```python\n",
    "def write_memory(state, config, store):\n",
    "```\n",
    "- **Purpose**: Analyzes the chat history and **saves new information** about the user in memory.\n",
    "\n",
    "**Key steps:**\n",
    "1. **Retrieve existing memory** (if any).  \n",
    "2. Format a prompt asking the model to **extract new information** from the chat.  \n",
    "3. **Save the updated memory** in the store, overwriting old data.\n",
    "\n",
    "#### Build the Workflow (Graph)\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "```\n",
    "- **StateGraph** defines the chatbot's process as a series of steps.\n",
    "\n",
    "**Nodes (steps):**\n",
    "1. **\"call_model\"**: Generate a response using memory.  \n",
    "2. **\"write_memory\"**: Update memory after responding.  \n",
    "\n",
    "**Edges (connections):**\n",
    "- Flow starts at **START**, then:\n",
    "  - Goes to **call_model**, generates a response.  \n",
    "  - Moves to **write_memory** to update memory.  \n",
    "  - Ends at **END**.\n",
    "\n",
    "#### Memory Management\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "- **InMemoryStore**: Saves memory across conversations (long-term storage).  \n",
    "\n",
    "```python\n",
    "within_thread_memory = MemorySaver()\n",
    "```\n",
    "- **MemorySaver**: Handles memory within a single session (short-term storage).\n",
    "\n",
    "#### Compile and Visualize the Workflow\n",
    "```python\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "```\n",
    "- Combines the steps into a **working chatbot pipeline** with memory support.\n",
    "\n",
    "```python\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "```\n",
    "- **Visualizes** the workflow graph, showing nodes and connections.\n",
    "\n",
    "#### What Does It Do?\n",
    "1. **Remembers user information** like preferences, interests, or goals between sessions.  \n",
    "2. **Personalizes responses** based on stored memory.  \n",
    "3. **Updates memory dynamically** after each chat session.  \n",
    "\n",
    "#### Real-Life Example:\n",
    "**Input 1:**  \n",
    "> \"Hi, I’m John and I love hiking!\"  \n",
    "\n",
    "**Response 1:**  \n",
    "> \"Nice to meet you, John! Hiking is a great hobby. Do you have a favorite trail?\"  \n",
    "\n",
    "**Memory Updated:**  \n",
    "- Name: John  \n",
    "- Hobby: Hiking  \n",
    "\n",
    "**Input 2 (next session):**  \n",
    "> \"I’m planning a trip to the mountains.\"  \n",
    "\n",
    "**Response 2 (uses memory):**  \n",
    "> \"That sounds perfect for someone who loves hiking, John! Which mountains are you planning to visit?\"  \n",
    "\n",
    "This interaction shows how memory is used to **enhance conversations** and **personalize responses** based on prior chats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c310857-fbd0-4009-bb8c-82f7795a95a3",
   "metadata": {},
   "source": [
    "## When we interact with the chatbot, we supply two things:\n",
    "\n",
    "1. `Short-term (within-thread) memory`: A `thread ID` for persisting the chat history.\n",
    "2. `Long-term (cross-thread) memory`: A `user ID` to namespace long-term memories to the user.\n",
    "\n",
    "Let's see how these work together in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b83227a-718c-436c-afb5-b1ae9fa02c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c918dbca-20e5-475f-ae41-bf23999d3413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive a vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun, Julio! San Francisco is such a beautiful city, and riding a Vespa must be a great way to explore it. Do you have any favorite spots you like to visit while riding around?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to drive a vespa around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46bc50-079e-4511-bba6-f94e9b44bdfb",
   "metadata": {},
   "source": [
    "* We're using the `MemorySaver` checkpointer for within-thread memory.\n",
    "* This saves the chat history to the thread.\n",
    "* We can look at the chat history saved to the thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce41ce3d-ef55-46e3-87ee-f43c76100a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Julio! It's nice to meet you. How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive a vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun, Julio! San Francisco is such a beautiful city, and riding a Vespa must be a great way to explore it. Do you have any favorite spots you like to visit while riding around?\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4f64c-5a0d-40a5-9dcd-411f12e745be",
   "metadata": {},
   "source": [
    "## Let's explain in simple terms what we just did\n",
    "\n",
    "The previous code demonstrates how the **chatbot with memory** works by processing user inputs and updating both **short-term** and **long-term memory**. Here’s a simple explanation step-by-step:\n",
    "\n",
    "#### Set Configuration for Memory\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`:** Used for **short-term memory** within a single session (temporary memory).  \n",
    "- **`user_id`:** Used for **long-term memory** across multiple sessions (persistent memory).  \n",
    "- Both IDs are set to **\"1\"** to identify the user and thread.\n",
    "\n",
    "#### Process First Input Message\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "```\n",
    "- **User Input:** \"Hi, my name is Julio.\"  \n",
    "- This is the **first message** sent to the chatbot.\n",
    "\n",
    "**Run the Graph (Workflow):**\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Processes the input** through the chatbot’s workflow graph:\n",
    "  1. **Fetches existing memory.**  \n",
    "  2. **Personalizes the response** based on memory.  \n",
    "  3. **Updates memory** to include new information (e.g., Julio’s name).  \n",
    "- **Outputs the chatbot’s response** by printing it to the screen.\n",
    "\n",
    "#### Process Second Input Message\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"I like to drive a vespa around San Francisco\")]\n",
    "```\n",
    "- **User Input:** \"I like to drive a vespa around San Francisco.\"  \n",
    "- This is the **second message** sent to the chatbot.\n",
    "\n",
    "**Run the Graph (Workflow):**\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Processes the second input** in the same way:\n",
    "  1. **Fetches updated memory** (now knows the user’s name is Julio).  \n",
    "  2. **Personalizes the response** by referring to previous information.  \n",
    "  3. **Updates memory** with new details (likes driving a Vespa in San Francisco).  \n",
    "- **Outputs the chatbot’s response** using the new memory.\n",
    "\n",
    "#### Review Stored Memory (Short-Term)\n",
    "```python\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]: \n",
    "    m.pretty_print()\n",
    "```\n",
    "- **Accesses the current session’s state** using the thread ID.  \n",
    "- **Retrieves all messages** exchanged so far (chat history).  \n",
    "- **Prints the conversation** for review.\n",
    "\n",
    "#### What Happens Behind the Scenes?\n",
    "\n",
    "1. **First Input (Julio's Name):**  \n",
    "   - No prior memory, so it’s stored: *\"Name: Julio.\"*  \n",
    "   - Response might acknowledge the name: *\"Hi Julio! How can I assist you today?\"*\n",
    "\n",
    "2. **Second Input (Vespa in San Francisco):**  \n",
    "   - Memory already includes *\"Name: Julio.\"*  \n",
    "   - New info added: *\"Likes to drive a Vespa in San Francisco.\"*  \n",
    "   - Response might say: *\"That sounds like a fun way to explore San Francisco, Julio!\"*\n",
    "\n",
    "3. **Final Review:**  \n",
    "   - Prints all chat messages and shows the current memory used during the session.\n",
    "\n",
    "#### Example Final Memory\n",
    "```\n",
    "- Name: Julio\n",
    "- Likes to drive a Vespa\n",
    "- Location: San Francisco\n",
    "```\n",
    "The chatbot can now **use this memory** in future conversations without asking for repeated details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3a126-057e-46b9-a65d-54980820f74e",
   "metadata": {},
   "source": [
    "## Now we can check if the memory was saved to the store\n",
    "* Recall that we compiled the graph with the store: \n",
    "\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "\n",
    "* And, we added a node to the graph (`write_memory`) that reflects on the chat history and saves a memory to the store.\n",
    "* Let's see if the memory was saved to the store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da70a3f-6411-4f0b-a9e4-aa845ae6418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'memory': \"**Updated User Information:**\\n- User's name is Julio.\\n- Likes to drive a Vespa.\\n- Rides around San Francisco.\"},\n",
       " 'key': 'user_memory',\n",
       " 'namespace': ['memory', '1'],\n",
       " 'created_at': '2025-01-30T15:49:39.048129+00:00',\n",
       " 'updated_at': '2025-01-30T15:49:39.048133+00:00'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703742e-edb7-490a-aa95-68ca2f7f7a86",
   "metadata": {},
   "source": [
    "## Now, let's use a different thread_id to simulate a new conversation with the same User\n",
    "* We will use a *new thread* with the *same user ID*.\n",
    "* We should see that the chatbot remembered the user's profile and used it to personalize the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfd7371-6a3f-4687-adbd-52d8e1044fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! Where would you recommend I drive with my vehicle in my town?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! Since you enjoy riding your Vespa around San Francisco, there are some fantastic routes you might love. Consider taking a ride through the scenic views of the Golden Gate Bridge, or perhaps a leisurely drive through Golden Gate Park. The winding roads of Lombard Street can be a fun challenge, and the Embarcadero offers beautiful waterfront views. If you're up for a bit of a climb, Twin Peaks provides a stunning panoramic view of the city. Enjoy your ride!\n"
     ]
    }
   ],
   "source": [
    "# We supply a user ID for across-thread memory as well as a new thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi! Where would you recommend I drive with my vehicle in my town?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2eab-8010-40c1-bb50-ac6eedef5360",
   "metadata": {},
   "source": [
    "* Excellent. As you can see, **the chatbot remembers our vehicle and our city from our previous conversation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e9fbb-cc0c-4acf-9c56-98ffbf2d9878",
   "metadata": {},
   "source": [
    "## Let's review what we just did\n",
    "\n",
    "The previous code demonstrates how the **chatbot with memory** handles a **new conversation thread** while still using the **same user memory**. Here's a simple explanation step-by-step:\n",
    "\n",
    "#### Set a New Configuration\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **`thread_id`: \"2\"** – Starts a **new conversation thread** (new session).  \n",
    "- **`user_id`: \"1\"** – Keeps the **same user ID**, so it can **access previously stored memory** (long-term memory).  \n",
    "\n",
    "> **Why this matters?**  \n",
    "Even though this is a **new session**, it still **remembers the user’s details** because the user ID hasn’t changed.\n",
    "\n",
    "#### Provide New User Input\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi! Where would you recommend I drive with my vehicle in my town?\")]\n",
    "```\n",
    "- **User Input:** The user asks for driving recommendations in their town.  \n",
    "- The chatbot will **personalize its response** based on what it **already knows** about the user.\n",
    "\n",
    "#### Process the Input with Memory\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Runs the chatbot workflow (graph):**  \n",
    "  1. **Fetches memory:** Looks up memory for **user_id = 1**.  \n",
    "  2. **Uses memory:** Knows the user’s name is **Julio** and that they like **driving a Vespa** in **San Francisco**.  \n",
    "  3. **Generates a response:** Suggests places relevant to **Vespa rides** in **San Francisco**.  \n",
    "  4. **Outputs the response.**\n",
    "\n",
    "#### What Happens Behind the Scenes?\n",
    "\n",
    "**Memory Used:**\n",
    "- **Name:** Julio  \n",
    "- **Likes to drive a Vespa**  \n",
    "- **Location:** San Francisco  \n",
    "\n",
    "**Example Response:**\n",
    "> \"Hi Julio! Since you enjoy driving a Vespa around San Francisco, I’d recommend exploring the scenic views along the Embarcadero or cruising through Golden Gate Park!\"\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "1. **Separate Sessions, Same Memory:**  \n",
    "   - The chatbot treats this as a **new conversation** (new thread).  \n",
    "   - It still **remembers previous details** because it uses the **same user ID**.\n",
    "\n",
    "2. **Personalized Responses:**  \n",
    "   - The chatbot uses **long-term memory** to make responses relevant, even in a new session.  \n",
    "\n",
    "3. **Practical Use:**  \n",
    "   - Allows users to have **multiple conversations** without repeating personal details.  \n",
    "   - Useful for chatbots designed to handle **ongoing relationships** with users.  \n",
    "\n",
    "#### Why is the New Thread Important?\n",
    "- **Thread ID = 1**: Might focus on introductions and learning about the user.  \n",
    "- **Thread ID = 2**: Focuses on travel recommendations without losing prior context.  \n",
    "\n",
    "This setup allows the chatbot to **manage multiple topics** while still knowing who the user is, creating a **seamless, personalized experience** across sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee179473-7601-45b9-b76c-33a9ab453417",
   "metadata": {},
   "source": [
    "## But, where is this long-term memory being saved? And, will this long-term memory persist if we close our application and open it again?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e4f3b-0971-46b3-9f39-c0e687af0142",
   "metadata": {},
   "source": [
    "#### Where is the long-term memory being saved?\n",
    "   - The long-term memory is saved in the `BaseStore` implementation used for `store`, which in your case is specified as `InMemoryStore()` and is assigned to `across_thread_memory`.\n",
    "\n",
    "   - **Key components for memory saving**:\n",
    "     - **Namespace and Key**: Memory is saved with a namespace (`(\"memory\", user_id)`) and a key (`\"user_memory\"`).\n",
    "     - **Memory content**: The memory content is stored as a dictionary, with the key `{\"memory\": <new_memory_content>}`.\n",
    "\n",
    "#### How is the memory persistent?\n",
    "   - The persistence of the memory depends on the implementation of `BaseStore`. In this case, `InMemoryStore` is a temporary, in-memory store that does not persist data to disk or any external storage.\n",
    "\n",
    "#### Would the memory persist if the app is closed and reopened?\n",
    "   - No, the memory **would not persist** if the app is closed and reopened, because `InMemoryStore` only holds the memory in the application's runtime memory. When the application shuts down, all data in `InMemoryStore` is lost.\n",
    "\n",
    "#### What is the application's runtime memory?\n",
    "\n",
    "The application runtime memory is like your computer's \"working space\" where all the temporary data and processes needed for an app to run are stored while the app is open. Think of it as a whiteboard:\n",
    "\n",
    "1. **Temporary Storage**: The runtime memory is used to hold information the app needs *right now*. For example, when you're typing a message in a chat app, the app stores your typed text in runtime memory before it's sent.\n",
    "\n",
    "2. **Cleared When Closed**: Just like wiping a whiteboard when you're done using it, runtime memory is cleared when the app is closed or your computer restarts. This is why, if an app doesn’t save its data to a more permanent place (like a file or a database), you lose everything stored in the runtime memory.\n",
    "\n",
    "3. **Depends on RAM**: Runtime memory lives in your computer’s **RAM (Random Access Memory)**, which is much faster than long-term storage (like your hard drive) but is also temporary.\n",
    "\n",
    "#### Example\n",
    "Imagine you're using a notepad app. If you write something but don’t save it, and then close the app, the text is gone forever. That’s because the text was only stored in runtime memory and wasn’t saved to permanent storage (like your computer’s hard drive). If you hit \"Save,\" the text is written to permanent storage, so you can reopen it later.\n",
    "\n",
    "#### Making the memory persistent across sessions\n",
    "To make the long-term memory in the provided LangGraph code persistent and saved to your hard disk, you would need to replace the InMemoryStore (which only keeps data in runtime memory) with an alternative that writes to persistent storage, such as a SQLite database, a file, or another form of long-term storage. This is still a very young field in LangGraph, we will add more about it as soon as we see a robust implementation in the LangGraph documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816085d-e823-4d84-aa7e-091420e5d758",
   "metadata": {},
   "source": [
    "## And, with that, we could continue our conversation\n",
    "* For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f90956a-2ea9-4cd3-b4f3-a00df110e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Great, are there any Whole Foods stores nearby that I can check out? I like a health food after driving.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Absolutely, Julio! There are several Whole Foods locations in San Francisco where you can grab some healthy food after your Vespa rides. Here are a few options:\n",
      "\n",
      "1. **Whole Foods Market on Market Street** - Located at 399 4th Street, it's right in the SoMa area and quite accessible.\n",
      "2. **Whole Foods Market on California Street** - Situated at 1765 California Street, this one is in the Pacific Heights neighborhood.\n",
      "3. **Whole Foods Market on Haight Street** - Found at 690 Stanyan Street, it's near Golden Gate Park, which could be convenient if you're riding around there.\n",
      "\n",
      "These stores should have a great selection of health foods for you to enjoy. Safe travels!\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Great, are there any Whole Foods stores nearby that I can check out? I like a health food after driving.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 025-store.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 025-store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
