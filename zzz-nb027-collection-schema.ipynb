{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a43ce59-8c2a-43fa-b830-e2f32f2300ec",
   "metadata": {},
   "source": [
    "# Using complex Memory Schemas for advanced operations (2): Saving memories in collections.\n",
    "* In the previous exercise learned how to save memories into a single user profile, but what if we want to save memories to a collection rather than single profile? We will learn how to do it in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29df6ce-9efb-404d-b3f5-567787ae153e",
   "metadata": {},
   "source": [
    "## What is a collection and why we would want to use it?\n",
    "\n",
    "Let’s clarify what this means step-by-step:\n",
    "\n",
    "#### Single User Profile\n",
    "In the previous exercise, we saved all the **memories** (pieces of information) into a **single profile** for one user.  \n",
    "\n",
    "For example:\n",
    "- User 1 has a profile with this memory:\n",
    "  ```\n",
    "  \"User likes learning about French.\"\n",
    "  ```\n",
    "- All the memories are stored **in one place**—just for **one user**.\n",
    "\n",
    "#### Collection\n",
    "Now, this exercise is introducing the idea of saving **memories to a collection** instead of tying them to a **single profile**.  \n",
    "\n",
    "A **collection** is like a **folder** or a **database** that can store **memories for multiple users**.\n",
    "\n",
    "Example:\n",
    "- Memory collection for **User 1**:\n",
    "  ```\n",
    "  [\"User likes learning about French.\", \"User prefers online courses.\"]\n",
    "  ```\n",
    "\n",
    "- Memory collection for **User 2**:  \n",
    "  ```\n",
    "  [\"User wants to learn Spanish.\", \"User prefers in-person classes.\"]\n",
    "  ```\n",
    "\n",
    "#### Key Difference\n",
    "- **Single Profile:** Stores memories for **one user only**.  \n",
    "- **Collection:** Stores **memories for multiple users** or **multiple topics** in an organized way.\n",
    "\n",
    "#### Why Use a Collection?\n",
    "A collection makes it easier to:\n",
    "1. Store memories for **multiple users**.  \n",
    "2. **Retrieve specific memories** for a given user.  \n",
    "3. **Scale the system** as more users or memories are added.  \n",
    "\n",
    "#### Analogy\n",
    "- **Single Profile** = A notebook for **one student**.  \n",
    "- **Collection** = A **bookshelf** with separate notebooks for **many students**.\n",
    "\n",
    "This approach is useful when your app or system needs to handle **more than one user** or **more complex data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065e336-d054-412c-8a3f-1fbec63e1bcd",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda8d4-80cf-4b8f-9981-94edda5e9911",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 027-collection-schema.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-langgraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e766aa-f3e2-491f-be99-d0c6b700d47a",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df978ec5-bfd2-4167-bd33-86bc2687d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a54d27-ea5a-49a3-9b4f-bef1d9a1620d",
   "metadata": {},
   "source": [
    "## Our goal: store the memories about user interactions in a collection\n",
    "* Instead of storing the user information in a fixed profile structure, we want to create a **flexible collection schema** to store memories about user interactions.\n",
    "    * Each memory will be stored as a separate entry with a single `content` field for the main information we want to remember.\n",
    "* This approach allows us to build an open-ended collection of memories that **can grow and change as we learn more about the user**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400d299-3489-4aa8-8657-439754425233",
   "metadata": {},
   "source": [
    "## Let's start by defining the collection schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76db7850-d968-4352-8cb6-c9f8615d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee92a5-162b-4214-a172-88aec97a2c60",
   "metadata": {},
   "source": [
    "## OK. Let's review what we just did\n",
    "\n",
    "The previous code defines two classes using **Pydantic**, a Python library for data validation and settings management. Here's a simple explanation:  \n",
    "\n",
    "#### Class `Memory`\n",
    "- Represents a **single piece of information** (memory) about the user.  \n",
    "- Contains **one attribute**:  \n",
    "  - **`content`** (a string) – Stores the actual memory, like \"User wants to learn French.\"  \n",
    "  - Includes a **description** to explain what this field is used for.  \n",
    "\n",
    "#### Class `MemoryCollection`:\n",
    "- Represents a **collection of memories** about the user.  \n",
    "- Contains **one attribute**:  \n",
    "  - **`memories`** – A **list of `Memory` objects** to store multiple pieces of information about the user.  \n",
    "  - Also has a **description** explaining its purpose.  \n",
    "\n",
    "#### Why Use Pydantic?\n",
    "- **Validation:** Ensures data follows the expected format (e.g., `content` must be a string).  \n",
    "- **Documentation:** Automatically generates clear documentation for the data model.  \n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "# Create a single memory\n",
    "memory1 = Memory(content=\"User expressed interest in learning about French.\")\n",
    "\n",
    "# Create another memory\n",
    "memory2 = Memory(content=\"User likes online courses.\")\n",
    "\n",
    "# Collect memories into a list\n",
    "memory_collection = MemoryCollection(memories=[memory1, memory2])\n",
    "\n",
    "# Access data\n",
    "print(memory_collection.memories[0].content)  # Output: User expressed interest in learning about French.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e2e16-780d-4315-931c-c12e9d24c5ac",
   "metadata": {},
   "source": [
    "## Now that we have defined the collection schema, let's check it out with a simple chatbot\n",
    "* We will use a chatbot with structured output like the one we used in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0554340-54d5-463c-b3a4-aef9ed3a3f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content=\"User's name is Julio.\"),\n",
       " Memory(content='Julio likes to drive his vespa in San Francisco.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_with_structure.invoke([HumanMessage(\"My name is Julio. I like to drive my vespa in SF.\")])\n",
    "memory_collection.memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001fa57-ae70-42ff-85fc-617d696b66e4",
   "metadata": {},
   "source": [
    "## OK. Let's now save the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d8b34f-08c6-435b-be31-548434e83c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58251895-7bce-493f-8c36-8c214c052737",
   "metadata": {},
   "source": [
    "## Finally, let's confirm we can retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5135b907-ac87-4e33-806a-5af7f1cb0f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '02ae72a3-7e7f-4020-9a74-3ebc8008590e', 'namespace': ['1', 'memories'], 'created_at': '2025-01-07T16:18:25.875274+00:00', 'updated_at': '2025-01-07T16:18:25.875280+00:00', 'score': None}\n",
      "{'value': {'content': 'Julio likes to drive his vespa in San Francisco.'}, 'key': '917a1e65-d413-404f-a74f-432c0c55a6d4', 'namespace': ['1', 'memories'], 'created_at': '2025-01-07T16:18:25.875489+00:00', 'updated_at': '2025-01-07T16:18:25.875490+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b0750-bcd0-4f0a-b175-e1c9c8b1e886",
   "metadata": {},
   "source": [
    "## Good. Let's review what we just did.\n",
    "\n",
    "The previous code builds on the previous example and demonstrates how to **generate structured data**, **store it in memory**, and **retrieve it later**. Here's a simple explanation:\n",
    "\n",
    "#### Import Libraries and Setup the Model \n",
    "```python\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "- Imports tools for working with chat messages and OpenAI's GPT models.  \n",
    "\n",
    "```python\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- Initializes the GPT-4o model with a temperature of **0** (ensures deterministic outputs).  \n",
    "\n",
    "\n",
    "#### Bind Schema to the Model\n",
    "```python\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "```\n",
    "- Configures the model to produce output in the format defined by the **`MemoryCollection`** schema created earlier.  \n",
    "- This ensures responses are **structured** and can be easily processed.  \n",
    "\n",
    "\n",
    "#### Generate Structured Output \n",
    "```python\n",
    "memory_collection = model_with_structure.invoke(\n",
    "    [HumanMessage(\"My name is Julio. I like to drive my vespa in SF.\")]\n",
    ")\n",
    "memory_collection.memories\n",
    "```\n",
    "- Sends a **user message** to the model and asks it to generate a structured response.  \n",
    "- The output is stored as a **`MemoryCollection`** object, following the schema defined earlier.  \n",
    "- For example:\n",
    "  - Memory 1: `\"My name is Julio.\"`  \n",
    "  - Memory 2: `\"I like to drive my vespa in SF.\"`  \n",
    "\n",
    "\n",
    "#### Import and Setup In-Memory Storage\n",
    "```python\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "```\n",
    "- Imports tools for **in-memory storage** (temporary storage for fast access).  \n",
    "- Creates a storage instance (`in_memory_store`) to save and manage memories.  \n",
    "\n",
    "\n",
    "#### Save Memories in Storage  \n",
    "```python\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "```\n",
    "- Defines a **namespace** (like a folder) using a **user ID** and the label `\"memories\"`.  \n",
    "- This organizes memories by user and type.  \n",
    "\n",
    "```python\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "```\n",
    "- Generates a **unique key** for each memory using `uuid`.  \n",
    "- Converts each memory into a **dictionary format** for storage.  \n",
    "- Saves the memory in the namespace with its key and value. \n",
    "\n",
    "\n",
    "#### Search and Retrieve Memories\n",
    "```python\n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())\n",
    "```\n",
    "- Searches for all memories stored in the namespace.  \n",
    "- Prints them as dictionaries, making them easy to read or process further.\n",
    "\n",
    "\n",
    "#### Example Output  \n",
    "```\n",
    "{'content': 'My name is Julio.'}\n",
    "{'content': 'I like to drive my vespa in SF.'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ae7d1-8e3a-4d31-b94e-3379025d4b13",
   "metadata": {},
   "source": [
    "## Adding and editing memories in the collection using TrustCall\n",
    "* Remember that in the last exercise we learned how to use TrustCall.\n",
    "* We will use it again here to add and edit the memories in our collection.\n",
    "* As you see below, we will set `enable_inserts=True` to allow the TrustCall extractor to insert new memories to the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344bedd2-dcd3-46c0-84dc-dedf60df9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0987441b-7f74-4758-9469-7da0bc08d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice vespa ride in San Francisco.\")]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69174615-5604-4c52-b271-d04fd7ac6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_Zq95MSs0HxLY2V3CZoLRDswM)\n",
      " Call ID: call_Zq95MSs0HxLY2V3CZoLRDswM\n",
      "  Args:\n",
      "    content: Julio had a nice vespa ride in San Francisco this morning.\n"
     ]
    }
   ],
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c66ac5-d507-4965-864e-d25144b13385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julio had a nice vespa ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02be1a64-a251-42e8-8891-59160dcbd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_Zq95MSs0HxLY2V3CZoLRDswM'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a5e59a-478a-4dd8-b92a-b84d6b66679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'Julio had a nice vespa ride in San Francisco this morning.'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [AIMessage(content=\"That's great, what did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Whole Foods and bought a green pie soup.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my trip to Carmel, and going back this weekend.\"),]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5766c9-89bf-4341-99df-ed3c505f5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e7cd37-3a60-44e7-9852-c1eaee95cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_G5ZsqCzkpJLnocBmTKvp4LlK)\n",
      " Call ID: call_G5ZsqCzkpJLnocBmTKvp4LlK\n",
      "  Args:\n",
      "    content: Julio had a nice vespa ride in San Francisco this morning. Then, he went to Whole Foods and bought a green pie soup.\n",
      "  Memory (call_aZ0NAlikwEv4USQVxk1f7Ytj)\n",
      " Call ID: call_aZ0NAlikwEv4USQVxk1f7Ytj\n",
      "  Args:\n",
      "    content: Julio was thinking about his trip to Carmel and considering going back this weekend.\n"
     ]
    }
   ],
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b54f85-85d1-4f12-b6bd-5d88a566ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julio had a nice vespa ride in San Francisco this morning. Then, he went to Whole Foods and bought a green pie soup.'\n",
      "content='Julio was thinking about his trip to Carmel and considering going back this weekend.'\n"
     ]
    }
   ],
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b073903a-9547-4e98-80d5-73118e848095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_G5ZsqCzkpJLnocBmTKvp4LlK', 'json_doc_id': '0'}\n",
      "{'id': 'call_aZ0NAlikwEv4USQVxk1f7Ytj'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff16551-aca4-4db3-acf0-a345c6fb8d18",
   "metadata": {},
   "source": [
    "## Let's review what we just did\n",
    "\n",
    "The previous code demonstrates how to use **TrustCall** to **extract, edit, and add memories** from a conversation using a structured format. Here's a simplified explanation step-by-step:\n",
    "\n",
    "#### Import TrustCall and Create an Extractor  \n",
    "```python\n",
    "from trustcall import create_extractor\n",
    "```\n",
    "- Imports the **TrustCall library**, which helps extract structured information from text.  \n",
    "\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "```\n",
    "- **Creates an extractor** tied to the GPT model.  \n",
    "- **`tools=[Memory]`:** Specifies that the output should match the **`Memory` schema** defined earlier.  \n",
    "- **`tool_choice=\"Memory\"`:** Ensures the extractor always uses the **`Memory` tool** to generate structured outputs.  \n",
    "- **`enable_inserts=True`:** Allows adding **new memories** to the collection during updates.  \n",
    "\n",
    "\n",
    "#### Provide Instructions and Initial Conversation\n",
    "```python\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "```\n",
    "- Provides **instructions** to the extractor, asking it to **identify relevant memories** from a conversation.  \n",
    "\n",
    "```python\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Julio.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Julio.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice vespa ride in San Francisco.\")]\n",
    "```\n",
    "- Represents the initial **conversation history** as a list of messages.  \n",
    "- **HumanMessage** and **AIMessage** distinguish between user and AI messages.  \n",
    "\n",
    "\n",
    "#### Extract Memories from the Conversation \n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})\n",
    "```\n",
    "- Combines the **instruction** and **conversation** into a **single input** for the extractor.  \n",
    "- The extractor **analyzes the messages** and generates structured memories based on the schema.  \n",
    "\n",
    "```python\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "```\n",
    "- Prints **intermediate steps** showing how the extractor interpreted the input.  \n",
    "\n",
    "```python\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Displays the **final extracted memories** that match the schema, e.g.:  \n",
    "  - **Memory 1:** `\"Hi, I'm Julio.\"`  \n",
    "  - **Memory 2:** `\"I had a nice vespa ride in San Francisco.\"`  \n",
    "\n",
    "```python\n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Outputs **metadata**, such as which tool (schema) was used and how the memories were processed.  \n",
    "\n",
    "\n",
    "#### Update the Conversation and Memories  \n",
    "\n",
    "**New Conversation:**  \n",
    "```python\n",
    "updated_conversation = [AIMessage(content=\"That's great, what did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Whole Foods and bought a green pie soup.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my trip to Carmel, and going back this weekend.\")]\n",
    "```\n",
    "- Adds **new context** to the conversation for further memory extraction or updates.  \n",
    "\n",
    "**Update Instruction:**\n",
    "```python\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "```\n",
    "- Updates the **instruction**, asking the extractor to **modify existing memories** or **add new ones** based on the updated conversation.  \n",
    "\n",
    "**Save Existing Memories:**\n",
    "```python\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "```\n",
    "- Prepares **existing memories** for the extractor, converting them into a format that includes:  \n",
    "  - **ID** (e.g., memory 0, memory 1).  \n",
    "  - **Tool name** (e.g., \"Memory\").  \n",
    "  - **Value** (the actual memory content).  \n",
    "\n",
    "\n",
    "#### Update Memories Using TrustCall\n",
    "```python\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})\n",
    "```\n",
    "- Processes the updated conversation along with the **existing memories** to:  \n",
    "  1. **Edit old memories** if necessary.  \n",
    "  2. **Insert new memories** based on the updated context.  \n",
    "\n",
    "**Inspect Results:** \n",
    "```python\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "```\n",
    "- Shows **details of the updates** made to the memories.  \n",
    "\n",
    "```python\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Displays the **new and updated memories** in the structured format.  \n",
    "  - **Memory 1:** `\"Julio went to Whole Foods and bought a green pie soup.\"`  \n",
    "  - **Memory 2:** `\"Julio is planning a trip to Carmel this weekend.\"`  \n",
    "\n",
    "```python\n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)\n",
    "```\n",
    "- Outputs **metadata** about the changes made (e.g., insertions, updates).  \n",
    "\n",
    "\n",
    "#### Example Output for Updated Memories\n",
    "```\n",
    "{'content': \"Julio went to Whole Foods and bought a green pie soup.\"}\n",
    "{'content': \"Julio is planning a trip to Carmel this weekend.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1ac3-e474-41ec-ac8f-597c5244e800",
   "metadata": {},
   "source": [
    "## OK, now let's try this approach with the chatbot we built in the last exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37561fc6-a6d5-4068-b394-8bfe7c507c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import merge_message_runs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "{memory}\"\"\"\n",
    "\n",
    "# Trustcall instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                        \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df69b1d4-fa4d-4828-a231-e5cd10aea38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Julio\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Julio! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2f3e41a-18e6-4b62-bae7-4559b292fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to drive my vespa around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun! San Francisco must be a great place to explore on a Vespa with all its unique neighborhoods and scenic views. Do you have any favorite spots you like to visit?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to drive my vespa around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e3d3656-626c-4999-92e9-9697b704aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '173a1f17-2308-4f16-bb04-9e20ea2f87fc', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:09.245031+00:00', 'updated_at': '2025-01-08T09:19:09.245032+00:00', 'score': None}\n",
      "{'value': {'content': 'User likes to drive their Vespa around San Francisco.'}, 'key': '75e9347b-19d7-462b-a662-114209d76729', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:34.470361+00:00', 'updated_at': '2025-01-08T09:19:34.470364+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e451c70-8ef4-4a9a-b2f8-7c627ae92c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I also enjoy going to Whole Foods\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Whole Foods is a great place for fresh and organic groceries. Do you have any favorite items you like to pick up when you're there?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to Whole Foods\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bf047-dff7-4dbf-b724-60bf7ee18383",
   "metadata": {},
   "source": [
    "#### Pay attention: now we will continue the conversation with a new thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f9d43fa-0df8-4bfe-90f9-7b74508525d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you recommend for me to buy in Whole Foods?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Since you enjoy going to Whole Foods, you might want to explore some of their fresh produce or organic options. If you're looking for something new, you could try their seasonal fruits and vegetables, or perhaps check out their selection of artisanal cheeses and freshly baked bread. If you're into cooking, their spice and herb section is also worth a look. Do you have any specific preferences or dietary needs?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What do you recommend for me to buy in Whole Foods?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95bd7ced-b972-472d-9077-ecd3af90eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"User's name is Julio.\"}, 'key': '173a1f17-2308-4f16-bb04-9e20ea2f87fc', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:09.245031+00:00', 'updated_at': '2025-01-08T09:19:09.245032+00:00', 'score': None}\n",
      "{'value': {'content': 'User likes to drive their Vespa around San Francisco.'}, 'key': '75e9347b-19d7-462b-a662-114209d76729', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:19:34.470361+00:00', 'updated_at': '2025-01-08T09:19:34.470364+00:00', 'score': None}\n",
      "{'value': {'content': 'User enjoys going to Whole Foods.'}, 'key': '55ba8966-aee1-4582-aa9a-9fe2250a3ade', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:20:22.873059+00:00', 'updated_at': '2025-01-08T09:20:22.873061+00:00', 'score': None}\n",
      "{'value': {'content': 'User is interested in recommendations for purchases at Whole Foods.'}, 'key': '0e3e60e8-adbb-4818-8af4-c4feadca4592', 'namespace': ['memories', '1'], 'created_at': '2025-01-08T09:22:44.080135+00:00', 'updated_at': '2025-01-08T09:22:44.080136+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# PAY ATTENTION HERE: see how the second threat\n",
    "# is also saved in the user memory.\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb6d13-724e-4cc2-be17-1b1de9d86c12",
   "metadata": {},
   "source": [
    "## OK. Let's review what we just did\n",
    "\n",
    "The previous code demonstrates how to create an **AI chatbot with long-term memory** using **LangGraph** and **TrustCall**. The chatbot can **extract, store, update, and use memories** from conversations to make interactions more personalized. Here's a **simplified explanation**:\n",
    "\n",
    "\n",
    "#### 1. Key Components\n",
    "\n",
    "**1.1 Memory Schema**\n",
    "```python\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"Stores user information.\")\n",
    "```\n",
    "- Defines a **memory structure** to store important user details (e.g., \"Julio likes driving a vespa\").  \n",
    "\n",
    "\n",
    "**1.2 TrustCall Extractor**\n",
    "```python\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "```\n",
    "- Uses **TrustCall** to **extract and update memories** from conversations.  \n",
    "- **`enable_inserts=True`:** Allows adding **new memories** without deleting old ones.  \n",
    "\n",
    "\n",
    "**1.3 Instructions for AI Behavior**\n",
    "```python\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You have a long-term memory for user details.\"\"\"\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on the conversation and update or insert new memories.\"\"\"\n",
    "```\n",
    "- Guides the AI model to:\n",
    "  1. **Use memories** in its responses.  \n",
    "  2. **Update or create memories** after each interaction.  \n",
    "\n",
    "#### 2. Memory Handling Functions\n",
    "\n",
    "**2.1 Respond with Memory Context**\n",
    "```python\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "```\n",
    "- **Loads stored memories** for the user.  \n",
    "- **Personalizes responses** by including relevant memories (e.g., \"Julio likes vespa rides\").  \n",
    "\n",
    "\n",
    "**2.2 Update Memories**\n",
    "```python\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(item.key, tool_name, item.value) for item in existing_items] if existing_items else None)\n",
    "\n",
    "    updated_messages = list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \"existing\": existing_memories})\n",
    "\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace, rmeta.get(\"json_doc_id\", str(uuid.uuid4())), r.model_dump(mode=\"json\"))\n",
    "```\n",
    "- **Analyzes chat history** to extract new or updated memories.  \n",
    "- **Stores them** in memory for future use.  \n",
    "\n",
    "\n",
    "#### 3. Build the Chatbot Flow\n",
    "\n",
    "**3.1 Create Graph for Conversation Flow**\n",
    "```python\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "```\n",
    "- **Defines the flow** of the chatbot:  \n",
    "  1. Responds based on memory.  \n",
    "  2. Updates or inserts new memories after the response.  \n",
    "\n",
    "\n",
    "**3.2 Configure Memory Storage**\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "```\n",
    "- **Short-term memory:** Tracks chat history during the session.  \n",
    "- **Long-term memory:** Stores important user details for future sessions.  \n",
    "\n",
    "\n",
    "#### 4. Interactions and Memory Usage\n",
    "\n",
    "**4.1 Input Messages**\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Julio\")]\n",
    "```\n",
    "- The user introduces themselves, and this detail is **saved as memory**.  \n",
    "\n",
    "**4.2 Process and Respond**\n",
    "```python\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "- **Sends user input** through the chatbot flow.  \n",
    "- **Stores memory:** \"User's name is Julio.\"  \n",
    "\n",
    "\n",
    "**4.3 Store and Retrieve Memories**\n",
    "```python\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())\n",
    "```\n",
    "- **Retrieves stored memories** and prints them:\n",
    "  ```\n",
    "  {'content': 'User's name is Julio.'}\n",
    "  ```\n",
    "\n",
    "\n",
    "**4.4 Continue Conversations**\n",
    "```python\n",
    "input_messages = [HumanMessage(content=\"I like to drive my vespa around San Francisco\")]\n",
    "```\n",
    "- Adds more user details, saved as:\n",
    "  ```\n",
    "  {'content': 'User likes to drive a vespa around San Francisco.'}\n",
    "  ```\n",
    "\n",
    "**4.5 Switch Contexts**\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "```\n",
    "- **Switches conversation context** (e.g., different topic or session) while still **using stored memories** about Julio.  \n",
    "\n",
    "\n",
    "#### 5. Example Output\n",
    "\n",
    "**User Messages:**\n",
    "1. \"Hi, my name is Julio.\"  \n",
    "2. \"I like to drive my vespa around San Francisco.\"  \n",
    "3. \"I also enjoy going to Whole Foods.\"  \n",
    "\n",
    "\n",
    "**Stored Memories:**\n",
    "```\n",
    "{'content': \"User's name is Julio.\"}\n",
    "{'content': \"User likes to drive a vespa around San Francisco.\"}\n",
    "{'content': \"User enjoys going to Whole Foods.\"}\n",
    "```\n",
    "\n",
    "\n",
    "**Next Interaction:**\n",
    "User Input: \n",
    "\"What do you recommend for me to buy in Whole Foods?\"  \n",
    "\n",
    "Response (Personalized): \n",
    "\"Since you enjoy Whole Foods, you might like their organic soups or healthy snacks. Have you tried their green pie soup?\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 027-collection-schema.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 027-collection-schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
